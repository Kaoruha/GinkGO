# Upstream: Schedulerï¼ˆæ— çŠ¶æ€è°ƒåº¦å™¨ï¼Œä»Redisè¯»å–çŠ¶æ€ï¼‰
# Downstream: ExecutionNodeï¼ˆæ¥æ”¶è°ƒåº¦æŒ‡ä»¤ï¼Œæ›´æ–°Portfolioåˆ†é…ï¼‰
# Role: PortfolioåŠ¨æ€è°ƒåº¦å™¨ï¼Œè´Ÿè´£è´Ÿè½½å‡è¡¡ã€æ•…éšœæ£€æµ‹å’Œä¼˜é›…è¿ç§»


"""
Scheduler è°ƒåº¦å™¨ï¼ˆæ— çŠ¶æ€è®¾è®¡ï¼‰

Scheduler æ˜¯ LiveCore çš„è°ƒåº¦ç»„ä»¶ï¼Œè´Ÿè´£ï¼š
1. å®šæœŸæ‰§è¡Œè°ƒåº¦ç®—æ³•ï¼ˆæ¯30ç§’ï¼‰
2. ExecutionNode å¿ƒè·³æ£€æµ‹ï¼ˆæ¯10ç§’ä¸ŠæŠ¥ï¼ŒTTL=30ç§’ï¼‰
3. Portfolio åŠ¨æ€åˆ†é…åˆ° ExecutionNodeï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
4. ExecutionNode æ•…éšœæ—¶è‡ªåŠ¨è¿ç§» Portfolio
5. å‘å¸ƒè°ƒåº¦æ›´æ–°åˆ° Kafka schedule.updates topic
6. æ¥æ”¶å¹¶å¤„ç†æ¥è‡ª Kafka çš„å‘½ä»¤ï¼ˆç«‹å³è°ƒåº¦ã€é‡æ–°è®¡ç®—ç­‰ï¼‰

è®¾è®¡è¦ç‚¹ï¼š
- æ— çŠ¶æ€è®¾è®¡ï¼šæ‰€æœ‰è°ƒåº¦æ•°æ®å­˜å‚¨åœ¨ Redis
- å•çº¿ç¨‹æ¶æ„ï¼šé¿å…çº¿ç¨‹åµŒå¥—ï¼Œè°ƒåº¦å¾ªç¯å’Œå‘½ä»¤å¤„ç†åœ¨åŒä¸€ä¸ªçº¿ç¨‹
- æ°´å¹³æ‰©å±•ï¼šæ”¯æŒå¤šä¸ª Scheduler å®ä¾‹ï¼ˆé€šè¿‡ Redis åˆ†å¸ƒå¼é”ï¼‰
- æ•…éšœæ¢å¤ï¼šExecutionNode ç¦»çº¿æ—¶è‡ªåŠ¨è¿ç§» Portfolioï¼ˆ< 60ç§’ï¼‰
- ä¼˜é›…é‡å¯ï¼šé…ç½®æ›´æ–°æ—¶è§¦å‘ Portfolio ä¼˜é›…é‡å¯ï¼ˆ< 30ç§’ï¼‰
- å‘½ä»¤å“åº”ï¼šæ”¯æŒé€šè¿‡ Kafka æ¥æ”¶ä¸»åŠ¨è°ƒåº¦å‘½ä»¤ï¼ˆéé˜»å¡å¤„ç†ï¼‰

Redis æ•°æ®ç»“æ„ï¼š
- heartbeat:node:{node_id}           - ExecutionNode å¿ƒè·³ï¼ˆString, TTL=30ç§’ï¼‰
- schedule:plan                       - å½“å‰è°ƒåº¦è®¡åˆ’ï¼ˆHash, key=portfolio_id, value=node_idï¼‰
- node:{node_id}:portfolios           - Node ä¸Šçš„ Portfolio åˆ—è¡¨ï¼ˆSetï¼‰
- node:{node_id}:metrics              - Node æ€§èƒ½æŒ‡æ ‡ï¼ˆHash: portfolio_count, queue_size, cpu_usageï¼‰
- portfolio:{portfolio_id}:status     - Portfolio çŠ¶æ€ï¼ˆString: RUNNING/STOPPING/RELOADINGï¼‰

Kafka Topics:
- schedule.updates                    - è°ƒåº¦æ›´æ–°äº‹ä»¶ï¼ˆControlCommandï¼‰
- scheduler.commands                  - è°ƒåº¦å™¨å‘½ä»¤ï¼ˆç«‹å³è°ƒåº¦ã€é‡æ–°è®¡ç®—ã€è¿ç§»ç­‰ï¼‰

å‘½ä»¤æ ¼å¼ï¼ˆscheduler.commandsï¼‰:
{
  "command": "recalculate|schedule|migrate|pause|resume|status",
  "timestamp": "2026-01-06T12:00:00",
  "params": {...}
}

çº¿ç¨‹æ¨¡å‹ï¼š
- å•çº¿ç¨‹ï¼šè°ƒåº¦å¾ªç¯ + å‘½ä»¤å¤„ç†åœ¨åŒä¸€ä¸ªçº¿ç¨‹
- é¿å…çº¿ç¨‹åµŒå¥—ï¼šä¸å¯åŠ¨å­çº¿ç¨‹å¤„ç†å‘½ä»¤
- å‘½ä»¤æ£€æŸ¥æ—¶æœºï¼šæ¯æ¬¡è°ƒåº¦å¾ªç¯å + æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡

çŠ¶æ€ç®¡ç†ï¼š
- RUNNING: æ­£å¸¸è¿è¡Œï¼Œæ‰§è¡Œè°ƒåº¦å¾ªç¯å’Œå¤„ç†å‘½ä»¤
- PAUSED: æš‚åœè°ƒåº¦ï¼Œåªå¤„ç†å‘½ä»¤ï¼ˆä¸æ‰§è¡Œ_schedule_loopï¼‰
- STOPPED: åœæ­¢è¿è¡Œï¼Œé€€å‡ºä¸»å¾ªç¯

ä½¿ç”¨æ–¹å¼ï¼š
    from ginkgo.livecore.scheduler import Scheduler

    # æ–¹å¼1ï¼šä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆè‡ªåŠ¨åˆ›å»º Redis å’Œ Kafka è¿æ¥ï¼‰
    scheduler = Scheduler()
    scheduler.start()

    # æ–¹å¼2ï¼šè‡ªå®šä¹‰é…ç½®
    scheduler = Scheduler(
        redis_client=custom_redis_client,
        kafka_producer=custom_kafka_producer,
        schedule_interval=30  # 30ç§’è°ƒåº¦ä¸€æ¬¡
    )
    scheduler.start()
"""

import time
import threading
import logging
from typing import Dict, List, Optional, Set
from datetime import datetime

try:
    from redis import Redis
except ImportError:
    Redis = None

try:
    from ginkgo.data.drivers.ginkgo_kafka import GinkgoProducer
except ImportError:
    GinkgoProducer = None

from ginkgo.enums import SOURCE_TYPES
from ginkgo.libs.utils.common import time_logger, retry


# è·å–æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)


class Scheduler(threading.Thread):
    """
    Scheduler è°ƒåº¦å™¨ï¼ˆæ— çŠ¶æ€è®¾è®¡ï¼‰

    èŒè´£ï¼š
    - å®šæœŸæ‰§è¡Œè°ƒåº¦ç®—æ³•ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
    - æ£€æŸ¥ ExecutionNode å¿ƒè·³å¹¶æ£€æµ‹ç¦»çº¿
    - åˆ†é… Portfolio åˆ°å¥åº·çš„ ExecutionNode
    - å‘å¸ƒè°ƒåº¦æ›´æ–°åˆ° Kafka

    è®¾è®¡æ¨¡å¼ï¼š
    - æ— çŠ¶æ€è®¾è®¡ï¼šæ‰€æœ‰çŠ¶æ€å­˜å‚¨åœ¨ Redis
    - å®šæœŸè°ƒåº¦ï¼šæ¯ 30 ç§’æ‰§è¡Œä¸€æ¬¡è°ƒåº¦ç®—æ³•
    - å¿ƒè·³æ£€æµ‹ï¼šæ£€æŸ¥ Redis ä¸­çš„å¿ƒè·³é”®ï¼ˆTTL=30ç§’ï¼‰
    """

    # Redis é”®å‰ç¼€
    HEARTBEAT_PREFIX = "heartbeat:node:"
    SCHEDULE_PLAN_KEY = "schedule:plan"
    NODE_PORTFOLIOS_PREFIX = "node:"
    NODE_METRICS_PREFIX = "node:metrics:"
    PORTFOLIO_STATUS_PREFIX = "portfolio:"

    # é»˜è®¤é…ç½®
    DEFAULT_SCHEDULE_INTERVAL = 30  # 30ç§’è°ƒåº¦ä¸€æ¬¡
    STATUS_REPORT_INTERVAL = 1800  # 30åˆ†é’Ÿæ±‡æŠ¥ä¸€æ¬¡çŠ¶æ€ï¼ˆ1800ç§’ï¼‰
    HEARTBEAT_TTL = 30  # å¿ƒè·³ TTL 30ç§’
    MAX_PORTFOLIOS_PER_NODE = 5  # æ¯ä¸ª Node æœ€å¤šè¿è¡Œ 5 ä¸ª Portfolio
    COMMAND_TOPIC = "scheduler.commands"  # å‘½ä»¤ä¸»é¢˜

    def __init__(
        self,
        redis_client: Optional[Redis] = None,
        kafka_producer: Optional[GinkgoProducer] = None,
        schedule_interval: int = DEFAULT_SCHEDULE_INTERVAL,
        status_report_interval: int = STATUS_REPORT_INTERVAL,
        node_id: str = "scheduler_1"
    ):
        """
        åˆå§‹åŒ– Scheduler

        Args:
            redis_client: Redis å®¢æˆ·ç«¯ï¼ˆç”¨äºçŠ¶æ€å­˜å‚¨ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨åˆ›å»º
            kafka_producer: Kafka ç”Ÿäº§è€…ï¼ˆç”¨äºå‘å¸ƒè°ƒåº¦æ›´æ–°ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨åˆ›å»º
            schedule_interval: è°ƒåº¦é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 30 ç§’
            status_report_interval: çŠ¶æ€æ±‡æŠ¥é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 1800 ç§’ï¼ˆ30åˆ†é’Ÿï¼‰
            node_id: Scheduler èŠ‚ç‚¹ IDï¼ˆç”¨äºæ—¥å¿—æ ‡è¯†ï¼‰
        """
        super().__init__()

        # å¦‚æœæ²¡æœ‰ä¼ å…¥ Redis å®¢æˆ·ç«¯ï¼Œè‡ªåŠ¨åˆ›å»º
        if redis_client is None:
            from ginkgo.data.crud import RedisCRUD
            redis_crud = RedisCRUD()
            redis_client = redis_crud.redis

        # å¦‚æœæ²¡æœ‰ä¼ å…¥ Kafka ç”Ÿäº§è€…ï¼Œè‡ªåŠ¨åˆ›å»º
        if kafka_producer is None:
            kafka_producer = GinkgoProducer()

        self.redis_client = redis_client
        self.kafka_producer = kafka_producer
        self.schedule_interval = schedule_interval
        self.status_report_interval = status_report_interval
        self.node_id = node_id

        # çŠ¶æ€æ ‡å¿—
        self.is_running = False
        self.should_stop = False
        self.is_paused = False  # æš‚åœæ ‡å¿—
        self.last_status_report_time = 0  # ä¸Šæ¬¡çŠ¶æ€æ±‡æŠ¥æ—¶é—´

        # åˆ›å»ºå‘½ä»¤æ¶ˆè´¹è€…ï¼ˆåœ¨ä¸»çº¿ç¨‹ä¸­å¤„ç†å‘½ä»¤ï¼Œé¿å…çº¿ç¨‹åµŒå¥—ï¼‰
        try:
            from ginkgo.data.drivers.ginkgo_kafka import GinkgoConsumer
            self.command_consumer = GinkgoConsumer(
                topic=self.COMMAND_TOPIC,
                group_id=f"scheduler_{self.node_id}"
            )
            logger.debug(f"Command consumer created for {self.COMMAND_TOPIC}")
        except Exception as e:
            logger.warning(f"Failed to create command consumer: {e}")
            self.command_consumer = None

        logger.info(f"Scheduler {self.node_id} initialized (interval={schedule_interval}s, status_report={status_report_interval}s)")

    def run(self):
        """
        Scheduler ä¸»å¾ªç¯

        å®šæœŸæ‰§è¡Œè°ƒåº¦ç®—æ³•ï¼š
        1. æ£€æŸ¥ ExecutionNode å¿ƒè·³
        2. æ£€æµ‹ç¦»çº¿ Node
        3. é‡æ–°åˆ†é…ç¦»çº¿ Node çš„ Portfolio
        4. å‘å¸ƒè°ƒåº¦æ›´æ–°åˆ° Kafka
        5. æ£€æŸ¥å¹¶å¤„ç†ä¸»åŠ¨è°ƒåº¦å‘½ä»¤ï¼ˆéé˜»å¡ï¼‰

        è®¾è®¡è¦ç‚¹ï¼š
        - å•çº¿ç¨‹è®¾è®¡ï¼šè°ƒåº¦å¾ªç¯å’Œå‘½ä»¤å¤„ç†åœ¨åŒä¸€ä¸ªçº¿ç¨‹
        - é¿å…çº¿ç¨‹åµŒå¥—ï¼šä¸å¯åŠ¨å­çº¿ç¨‹å¤„ç†å‘½ä»¤
        - éé˜»å¡å‘½ä»¤æ£€æŸ¥ï¼šåœ¨æ¯æ¬¡è°ƒåº¦å¾ªç¯åæ£€æŸ¥å‘½ä»¤
        - æ”¯æŒæš‚åœï¼šPAUSED çŠ¶æ€ä¸‹ä¸æ‰§è¡Œè°ƒåº¦ï¼Œä½†ä»å¤„ç†å‘½ä»¤
        """
        self.is_running = True
        logger.info(f"Scheduler {self.node_id} started")

        # å‘é€å¯åŠ¨é€šçŸ¥
        try:
            from ginkgo.notifier.core.notification_service import notify
            notify(
                f"è°ƒåº¦å™¨ {self.node_id} å·²å¯åŠ¨",
                level="INFO",
                module="Scheduler",
                details={
                    "èŠ‚ç‚¹ID": self.node_id,
                    "è°ƒåº¦é—´éš”": f"{self.schedule_interval}ç§’"
                }
            )
        except Exception as e:
            logger.warning(f"Failed to send startup notification: {e}")

        while not self.should_stop:
            try:
                # 1. æ£€æŸ¥å¹¶å¤„ç†å‘½ä»¤ï¼ˆéé˜»å¡ï¼‰
                self._check_commands()

                # 2. å¦‚æœæœªæš‚åœï¼Œæ‰§è¡Œè°ƒåº¦ç®—æ³•
                if not self.is_paused:
                    self._schedule_loop()
                else:
                    logger.debug(f"Scheduler {self.node_id} is paused, skipping schedule loop")

                # 3. æ£€æŸ¥æ˜¯å¦éœ€è¦å‘é€çŠ¶æ€æ±‡æŠ¥ï¼ˆæ¯30åˆ†é’Ÿï¼‰
                current_time = time.time()
                if current_time - self.last_status_report_time >= self.status_report_interval:
                    self._send_status_report()
                    self.last_status_report_time = current_time

                # 4. ç­‰å¾…ä¸‹ä¸€æ¬¡è°ƒåº¦ï¼ˆå¯ä¸­æ–­ï¼‰
                for second in range(self.schedule_interval):
                    if self.should_stop:
                        break
                    time.sleep(1)

                    # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡å‘½ä»¤ï¼ˆæé«˜å“åº”é€Ÿåº¦ï¼‰
                    if second > 0 and second % 5 == 0:
                        self._check_commands()

            except Exception as e:
                logger.error(f"Scheduler {self.node_id} error: {e}")
                time.sleep(5)  # å‡ºé”™åç­‰å¾… 5 ç§’å†é‡è¯•

        self.is_running = False
        logger.info(f"Scheduler {self.node_id} stopped")

    def _schedule_loop(self):
        """
        æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„è°ƒåº¦å¾ªç¯

        æ­¥éª¤ï¼š
        1. è·å–æ‰€æœ‰ ExecutionNode çŠ¶æ€
        2. æ£€æŸ¥å¿ƒè·³ï¼Œè¿‡æ»¤ç¦»çº¿ Node
        3. å‘ç°æ–°çš„ live portfolioï¼ˆä»æ•°æ®åº“ï¼‰
        4. æ£€æµ‹ç¦»çº¿ Node çš„ Portfolioï¼ˆå­¤å„¿ï¼‰
        5. æ‰§è¡Œè´Ÿè½½å‡è¡¡ç®—æ³•
        6. å‘å¸ƒè°ƒåº¦æ›´æ–°
        """
        # 1. è·å–å¥åº·çš„ ExecutionNode
        healthy_nodes = self._get_healthy_nodes()

        # 2. è·å–å½“å‰è°ƒåº¦è®¡åˆ’
        current_plan = self._get_current_schedule_plan()

        # 3. å‘ç°æ–°çš„ live portfolioï¼ˆä»æ•°æ®åº“ï¼‰
        new_portfolios = self._discover_new_portfolios(current_plan)

        # 3.5. æ¸…ç†å·²åˆ é™¤çš„ portfolioï¼ˆè°ƒåº¦è®¡åˆ’æœ‰ï¼Œä½†æ•°æ®åº“ä¸å­˜åœ¨çš„ï¼‰
        deleted_portfolios = self._detect_deleted_portfolios(current_plan)
        if deleted_portfolios:
            logger.warning(f"Deleted portfolios detected: {[p[:8] for p in deleted_portfolios]}")

            # å‘é€åˆ é™¤é€šçŸ¥
            try:
                from ginkgo.notifier.core.notification_service import notify
                notify(
                    f"æ¸…ç†å·²åˆ é™¤Portfolio - {len(deleted_portfolios)}ä¸ªPortfolioå·²ä»è°ƒåº¦è®¡åˆ’ç§»é™¤",
                    level="INFO",
                    module="Scheduler",
                    details={
                        "å·²åˆ é™¤Portfolioæ•°": len(deleted_portfolios),
                        "Portfolio IDs": ", ".join([p[:8] for p in deleted_portfolios])
                    }
                )
            except Exception as e:
                logger.warning(f"Failed to send deleted portfolios notification: {e}")

            # ä»å½“å‰è®¡åˆ’ä¸­ç§»é™¤å·²åˆ é™¤çš„portfolio
            for portfolio_id in deleted_portfolios:
                if portfolio_id in current_plan:
                    del current_plan[portfolio_id]

            # ç«‹å³æ›´æ–°Redisï¼ˆç§»é™¤å·²åˆ é™¤çš„portfolioï¼‰
            self.redis_client.delete(self.SCHEDULE_PLAN_KEY)
            if current_plan:
                self.redis_client.hset(self.SCHEDULE_PLAN_KEY, mapping=current_plan)

            logger.info(f"Removed {len(deleted_portfolios)} deleted portfolios from schedule plan and updated Redis")

        # 4. æ£€æµ‹ç¦»çº¿ Node çš„ Portfolio
        orphaned_portfolios = self._detect_orphaned_portfolios(healthy_nodes)

        # ========== è°ƒåº¦ä¿¡æ¯æ‰“å° ==========
        logger.info("")
        logger.info("="*70)
        logger.info(f"Scheduler Cycle - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("="*70)

        # [DEBUG] æ‰“å°å½“å‰è°ƒåº¦è®¡åˆ’è¯¦æƒ…
        logger.info(f"[DEBUG] Current schedule plan ({len(current_plan)} portfolios):")
        for portfolio_id, node_id in current_plan.items():
            status_icon = "âŒ" if node_id == "__ORPHANED__" else "âœ…"
            logger.info(f"  {status_icon} {portfolio_id[:8]}... â†’ {node_id}")

        logger.info("-"*70)

        # æ‰“å°å¯è°ƒåº¦èŠ‚ç‚¹
        if healthy_nodes:
            logger.info(f"Available schedulable nodes: {len(healthy_nodes)}")
            for node in healthy_nodes:
                node_id = node['node_id']
                count = node['metrics']['portfolio_count']
                logger.info(f"  - {node_id}: {count}/{self.MAX_PORTFOLIOS_PER_NODE} portfolios")
        else:
            logger.warning("No available schedulable nodes (all nodes offline or no nodes registered)")

        # æ‰“å°å½“å‰æ‰€æœ‰portfolio
        all_portfolios = set(current_plan.keys()) | set(new_portfolios)
        if all_portfolios:
            logger.info(f"Total portfolios: {len(all_portfolios)}")
        else:
            logger.info("No portfolios found")

        # æ‰“å°éœ€è¦è°ƒæ•´çš„portfolio
        portfolios_to_assign = list(new_portfolios) + orphaned_portfolios
        if portfolios_to_assign:
            logger.info(f"Portfolios to assign: {len(portfolios_to_assign)}")
            logger.info(f"  - New: {len(new_portfolios)}")
            if new_portfolios:
                for p in new_portfolios:
                    logger.info(f"      - {p[:8]}... (from database)")
            logger.info(f"  - Orphaned (node offline): {len(orphaned_portfolios)}")
            if orphaned_portfolios:
                for p in orphaned_portfolios:
                    old_node = current_plan.get(p, "unknown")
                    logger.info(f"      - {p[:8]}... (was on {old_node})")
        else:
            logger.info("No portfolio assignments needed")
            logger.info(f"[DEBUG] new_portfolios={len(new_portfolios)}, orphaned_portfolios={len(orphaned_portfolios)}")

        logger.info("-"*70)

        # 5. é‡æ–°åˆ†é… Portfolioï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
        new_plan = self._assign_portfolios(
            healthy_nodes=healthy_nodes,
            current_plan=current_plan,
            orphaned_portfolios=portfolios_to_assign
        )

        # æ‰“å°è°ƒåº¦è®¡åˆ’å˜åŒ–
        if new_plan != current_plan:
            changes = self._get_plan_changes(current_plan, new_plan)
            logger.info(f"Schedule plan changes: {len(changes)}")
            for portfolio_id, (old_node, new_node) in changes.items():
                if old_node == "__ORPHANED__":
                    if new_node == "__ORPHANED__":
                        # ORPHANED â†’ ORPHANED (ç†è®ºä¸Šä¸åº”è¯¥å‡ºç°)
                        logger.info(f"  ? {portfolio_id[:8]}...: {old_node} â†’ {new_node} (UNKNOWN)")
                    else:
                        # ORPHANED â†’ node (å­¤å„¿é‡æ–°åˆ†é… OR æ–°portfolio)
                        logger.info(f"  + {portfolio_id[:8]}... â†’ {new_node} (REASSIGNED)")
                elif new_node == "__ORPHANED__":
                    # node â†’ ORPHANED (èŠ‚ç‚¹ç¦»çº¿ï¼Œå˜ä¸ºå­¤å„¿)
                    logger.info(f"  - {portfolio_id[:8]}... â† {old_node} (ORPHANED)")
                else:
                    # node â†’ node (è¿ç§»)
                    logger.info(f"  ~ {portfolio_id[:8]}...: {old_node} â†’ {new_node} (MIGRATED)")
        else:
            logger.info("No schedule plan changes")

        logger.info("="*70)
        logger.info("")

        # 6. å‘å¸ƒè°ƒåº¦æ›´æ–°
        if new_plan != current_plan:
            self._publish_schedule_update(current_plan, new_plan)

    def stop(self):
        """
        åœæ­¢ Scheduler - ä¼˜é›…å…³é—­æµç¨‹

        æ ¸å¿ƒç­–ç•¥ï¼š
        1. è®¾ç½®åœæ­¢æ ‡å¿—ï¼ˆé€šçŸ¥ä¸»å¾ªç¯é€€å‡ºï¼‰
        2. æ¸…ç† Redis å¿ƒè·³å’ŒçŠ¶æ€æ•°æ®
        3. å…³é—­ Kafka Consumer å’Œ Producer
        4. ç­‰å¾…ä¸»çº¿ç¨‹ç»“æŸ
        """
        logger.info(f"")
        logger.info(f"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        logger.info(f"ğŸ›‘ Stopping Scheduler {self.node_id}")
        logger.info(f"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

        # 0. è®¾ç½®åœæ­¢æ ‡å¿—ï¼ˆé€šçŸ¥ä¸»å¾ªç¯é€€å‡ºï¼‰
        logger.info(f"[Step 1] Setting stop flag...")
        self.should_stop = True
        self.is_running = False
        logger.info(f"  âœ… Stop flag set")

        # 1. æ¸…ç† Redis æ•°æ®ï¼ˆé‡è¦ï¼šè®© ExecutionNode çŸ¥é“ Scheduler ç¦»çº¿ï¼‰
        logger.info(f"[Step 2] Cleaning up Redis data...")
        self._cleanup_redis_data()

        # 2. å…³é—­ Kafka Consumerï¼ˆåœæ­¢æ¥æ”¶å‘½ä»¤ï¼‰
        logger.info(f"[Step 3] Closing Kafka Consumer...")
        if self.command_consumer:
            try:
                self.command_consumer.close()
                logger.info(f"  âœ… Command consumer closed")
            except Exception as e:
                logger.error(f"  âœ— Error closing command consumer: {e}")
        else:
            logger.info(f"  â„¹ï¸  No command consumer to close")

        # 3. å…³é—­ Kafka Producerï¼ˆåœæ­¢å‘é€è°ƒåº¦æ›´æ–°ï¼‰
        logger.info(f"[Step 4] Closing Kafka Producer...")
        if self.kafka_producer:
            try:
                self.kafka_producer.close()
                logger.info(f"  âœ… Kafka producer closed")
            except Exception as e:
                logger.error(f"  âœ— Error closing Kafka producer: {e}")
        else:
            logger.info(f"  â„¹ï¸  No Kafka producer to close")

        logger.info(f"")
        logger.info(f"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        logger.info(f"âœ… Scheduler {self.node_id} stopped gracefully")
        logger.info(f"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        logger.info(f"")

    def _cleanup_redis_data(self):
        """
        æ¸…ç† Redis ä¸­çš„ Scheduler ç›¸å…³æ•°æ®

        æ¸…ç†å†…å®¹ï¼š
        1. Scheduler å¿ƒè·³æ•°æ®ï¼ˆè™½ç„¶ Scheduler æ²¡æœ‰å¿ƒè·³ï¼Œä½†æ¸…ç†ä»¥é˜²å°†æ¥æ·»åŠ ï¼‰
        2. Scheduler çŠ¶æ€æ•°æ®
        """
        try:
            deleted_keys = []

            # æ¸…ç† Scheduler å¿ƒè·³ï¼ˆé¢„ç•™ï¼Œå½“å‰æ²¡æœ‰å¿ƒè·³æœºåˆ¶ï¼‰
            heartbeat_key = f"heartbeat:scheduler:{self.node_id}"
            try:
                if self.redis_client.exists(heartbeat_key):
                    self.redis_client.delete(heartbeat_key)
                    deleted_keys.append(heartbeat_key)
                    logger.info(f"  âœ… Deleted scheduler heartbeat: {heartbeat_key}")
            except Exception as e:
                logger.warning(f"  âš ï¸  Could not delete heartbeat key: {e}")

            # æ¸…ç† Scheduler çŠ¶æ€ï¼ˆé¢„ç•™ï¼Œå½“å‰æ²¡æœ‰çŠ¶æ€é”®ï¼‰
            state_key = f"scheduler:state:{self.node_id}"
            try:
                if self.redis_client.exists(state_key):
                    self.redis_client.delete(state_key)
                    deleted_keys.append(state_key)
                    logger.info(f"  âœ… Deleted scheduler state: {state_key}")
            except Exception as e:
                logger.warning(f"  âš ï¸  Could not delete state key: {e}")

            # é‡è¦è¯´æ˜ï¼š
            # - schedule:plan ä¸æ¸…ç†ï¼ˆè¿™æ˜¯å…¨å±€è°ƒåº¦è®¡åˆ’ï¼Œå…¶ä»– Scheduler å¯èƒ½éœ€è¦ï¼‰
            # - heartbeat:node:* ä¸æ¸…ç†ï¼ˆè¿™æ˜¯ ExecutionNode çš„å¿ƒè·³ï¼Œä¸æ˜¯ Scheduler çš„ï¼‰
            # - node:*:portfolios ä¸æ¸…ç†ï¼ˆè¿™æ˜¯ ExecutionNode çš„çŠ¶æ€ï¼Œä¸æ˜¯ Scheduler çš„ï¼‰

            if deleted_keys:
                logger.info(f"  âœ… Total keys deleted: {len(deleted_keys)}")
            else:
                logger.info(f"  â„¹ï¸  No scheduler-specific keys to delete")

        except Exception as e:
            logger.error(f"  âœ— Error cleaning up Redis data: {e}")

    # ========================================================================
    # å¿ƒè·³æ£€æµ‹
    # ========================================================================

    def _get_healthy_nodes(self) -> List[Dict]:
        """
        è·å–æ‰€æœ‰å¥åº·çš„ ExecutionNodeï¼ˆæœ‰å¿ƒè·³ï¼‰

        Returns:
            List[Dict]: å¥åº·çš„ Node åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« node_id å’Œ metrics
        """
        try:
            # æ‰«ææ‰€æœ‰å¿ƒè·³é”®
            heartbeat_keys = self.redis_client.keys(f"{self.HEARTBEAT_PREFIX}*")

            healthy_nodes = []
            for key in heartbeat_keys:
                # æå– node_id
                node_id = key.decode('utf-8').replace(self.HEARTBEAT_PREFIX, "")

                # è·å– Node æ€§èƒ½æŒ‡æ ‡
                metrics = self._get_node_metrics(node_id)

                healthy_nodes.append({
                    'node_id': node_id,
                    'metrics': metrics
                })

            logger.debug(f"Found {len(healthy_nodes)} healthy nodes")
            return healthy_nodes

        except Exception as e:
            logger.error(f"Failed to get healthy nodes: {e}")
            return []

    def _get_node_metrics(self, node_id: str) -> Dict:
        """
        è·å– Node æ€§èƒ½æŒ‡æ ‡

        Args:
            node_id: ExecutionNode ID

        Returns:
            Dict: æ€§èƒ½æŒ‡æ ‡ {portfolio_count, queue_size, cpu_usage}
        """
        try:
            key = f"{self.NODE_METRICS_PREFIX}{node_id}"
            metrics = self.redis_client.hgetall(key)

            return {
                'portfolio_count': int(metrics.get(b'portfolio_count', 0)),
                'queue_size': int(metrics.get(b'queue_size', 0)),
                'cpu_usage': float(metrics.get(b'cpu_usage', 0.0))
            }
        except Exception as e:
            logger.error(f"Failed to get metrics for node {node_id}: {e}")
            return {'portfolio_count': 0, 'queue_size': 0, 'cpu_usage': 0.0}

    # ========================================================================
    # è°ƒåº¦è®¡åˆ’ç®¡ç†
    # ========================================================================

    def _get_current_schedule_plan(self) -> Dict[str, str]:
        """
        è·å–å½“å‰è°ƒåº¦è®¡åˆ’

        Returns:
            Dict: {portfolio_id: node_id}
        """
        try:
            plan = self.redis_client.hgetall(self.SCHEDULE_PLAN_KEY)

            # è½¬æ¢ bytes åˆ° str
            return {
                k.decode('utf-8'): v.decode('utf-8')
                for k, v in plan.items()
            }
        except Exception as e:
            logger.error(f"Failed to get current schedule plan: {e}")
            return {}

    def _detect_orphaned_portfolios(self, healthy_nodes: List[Dict]) -> List[str]:
        """
        æ£€æµ‹ç¦»çº¿ Node çš„ Portfolioï¼ˆå­¤å„¿ Portfolioï¼‰

        Args:
            healthy_nodes: å¥åº·çš„ Node åˆ—è¡¨

        Returns:
            List[str]: éœ€è¦é‡æ–°åˆ†é…çš„ portfolio_id åˆ—è¡¨
        """
        try:
            # è·å–å½“å‰è°ƒåº¦è®¡åˆ’
            current_plan = self._get_current_schedule_plan()

            # å¥åº·çš„ Node ID é›†åˆ
            healthy_node_ids = {n['node_id'] for n in healthy_nodes}

            # æ‰¾å‡ºåˆ†é…ç»™ç¦»çº¿ Node çš„ Portfolio
            orphaned = []
            for portfolio_id, node_id in current_plan.items():
                if node_id not in healthy_node_ids:
                    orphaned.append(portfolio_id)

            # åªåœ¨å‘ç°å­¤å„¿portfolioæ—¶æ‰“å°
            if orphaned:
                logger.warning(f"Orphan portfolios (node offline): {[p[:8] for p in orphaned]}")

                # å‘é€èŠ‚ç‚¹ä¸‹çº¿è­¦å‘Šé€šçŸ¥
                try:
                    from ginkgo.notifier.core.notification_service import notify

                    # æ‰¾å‡ºç¦»çº¿çš„èŠ‚ç‚¹
                    offline_nodes = set()
                    for portfolio_id in orphaned:
                        old_node = current_plan.get(portfolio_id)
                        if old_node and old_node != "__ORPHANED__":
                            offline_nodes.add(old_node)

                    notify(
                        f"æ£€æµ‹åˆ°èŠ‚ç‚¹ä¸‹çº¿ - {len(offline_nodes)}ä¸ªèŠ‚ç‚¹ç¦»çº¿, {len(orphaned)}ä¸ªPortfolioéœ€è¦é‡æ–°åˆ†é…",
                        level="WARN",
                        module="Scheduler",
                        details={
                            "ç¦»çº¿èŠ‚ç‚¹æ•°": len(offline_nodes),
                            "ç¦»çº¿èŠ‚ç‚¹": ", ".join(list(offline_nodes)[:5]),
                            "å—å½±å“Portfolioæ•°": len(orphaned),
                            "éœ€è¦é‡æ–°åˆ†é…": "æ˜¯"
                        }
                    )
                except Exception as e:
                    logger.warning(f"Failed to send node offline warning: {e}")

            return orphaned

        except Exception as e:
            logger.error(f"Failed to detect orphaned portfolios: {e}")
            return []

    def _detect_deleted_portfolios(self, current_plan: Dict[str, str]) -> List[str]:
        """
        æ£€æµ‹å·²åˆ é™¤çš„ Portfolioï¼ˆè°ƒåº¦è®¡åˆ’ä¸­æœ‰ï¼Œä½†æ•°æ®åº“ä¸å­˜åœ¨çš„ï¼‰

        Args:
            current_plan: å½“å‰è°ƒåº¦è®¡åˆ’ {portfolio_id: node_id}

        Returns:
            List[str]: éœ€è¦ä»è°ƒåº¦è®¡åˆ’ä¸­ç§»é™¤çš„ portfolio_id åˆ—è¡¨
        """
        try:
            # è·å–æ•°æ®åº“ä¸­æ‰€æœ‰ live portfolio
            all_portfolios = self._get_all_portfolios()
            existing_portfolio_ids = {p.uuid for p in all_portfolios}

            # æ‰¾å‡ºè°ƒåº¦è®¡åˆ’ä¸­æœ‰ï¼Œä½†æ•°æ®åº“ä¸­ä¸å­˜åœ¨çš„ portfolio
            deleted = []
            for portfolio_id in current_plan.keys():
                if portfolio_id not in existing_portfolio_ids:
                    deleted.append(portfolio_id)

            # åªåœ¨å‘ç°å·²åˆ é™¤portfolioæ—¶æ‰“å°
            if deleted:
                logger.warning(f"Deleted portfolios (removed from database): {[p[:8] for p in deleted]}")

            return deleted

        except Exception as e:
            logger.error(f"Failed to detect deleted portfolios: {e}")
            return []

    # ========================================================================
    # è´Ÿè½½å‡è¡¡ç®—æ³•
    # ========================================================================

    def _assign_portfolios(
        self,
        healthy_nodes: List[Dict],
        current_plan: Dict[str, str],
        orphaned_portfolios: List[str]
    ) -> Dict[str, str]:
        """
        è´Ÿè½½å‡è¡¡ç®—æ³•ï¼šåˆ†é… Portfolio åˆ° ExecutionNode

        ç­–ç•¥ï¼š
        1. ä¿ç•™å½“å‰å·²æœ‰çš„åˆ†é…ï¼ˆé™¤é Node ç¦»çº¿ï¼‰
        2. ä¼˜å…ˆåˆ†é…åˆ°è´Ÿè½½æœ€ä½çš„ Node
        3. æ¯ä¸ª Node æœ€å¤šè¿è¡Œ MAX_PORTFOLIOS_PER_NODE ä¸ª Portfolio
        4. å¦‚æœæ²¡æœ‰å¯ç”¨ Nodeï¼Œè¿”å›ç©ºåˆ†é…ï¼ˆç­‰å¾…ä¸‹æ¬¡è°ƒåº¦ï¼‰

        Args:
            healthy_nodes: å¥åº·çš„ Node åˆ—è¡¨
            current_plan: å½“å‰è°ƒåº¦è®¡åˆ’ {portfolio_id: node_id}
            orphaned_portfolios: éœ€è¦é‡æ–°åˆ†é…çš„ portfolio_id åˆ—è¡¨

        Returns:
            Dict: æ–°çš„è°ƒåº¦è®¡åˆ’ {portfolio_id: node_id}
        """
        new_plan = {}

        # 1. ä¿ç•™å½“å‰å¥åº·çš„åˆ†é…
        healthy_node_ids = {n['node_id'] for n in healthy_nodes}
        for portfolio_id, node_id in current_plan.items():
            # è·³è¿‡å­¤å„¿portfolioï¼ˆç­‰å¾…åˆ†é…ï¼‰
            if node_id == "__ORPHANED__":
                continue
            if node_id in healthy_node_ids:
                new_plan[portfolio_id] = node_id

        # 2. é‡æ–°åˆ†é…å­¤å„¿ Portfolio
        if orphaned_portfolios:
            # å¦‚æœæ²¡æœ‰å¥åº·èŠ‚ç‚¹ï¼Œä¿ç•™å­¤å„¿portfolioåœ¨è®¡åˆ’ä¸­ï¼ˆæ ‡è®°ä¸ºç‰¹æ®Šå€¼ï¼‰
            if not healthy_nodes:
                for portfolio_id in orphaned_portfolios:
                    # Redisä¸èƒ½å­˜å‚¨Noneï¼Œä½¿ç”¨ç‰¹æ®Šå­—ç¬¦ä¸²æ ‡è®°
                    new_plan[portfolio_id] = "__ORPHANED__"
                    logger.warning(
                        f"Portfolio {portfolio_id[:8]}... marked as orphaned "
                        f"(waiting for available node)"
                    )
                return new_plan

            # æŒ‰è´Ÿè½½æ’åºï¼ˆè´Ÿè½½ä½çš„ä¼˜å…ˆï¼‰
            sorted_nodes = sorted(
                healthy_nodes,
                key=lambda n: n['metrics']['portfolio_count']
            )

            for portfolio_id in orphaned_portfolios:
                # æ‰¾åˆ°è´Ÿè½½æœ€ä½çš„ Node
                assigned = False
                for node in sorted_nodes:
                    node_id = node['node_id']
                    portfolio_count = node['metrics']['portfolio_count']

                    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡ä¸Šé™
                    if portfolio_count < self.MAX_PORTFOLIOS_PER_NODE:
                        new_plan[portfolio_id] = node_id
                        assigned = True

                        # æ›´æ–°è®¡æ•°ï¼ˆç”¨äºåç»­åˆ†é…ï¼‰
                        node['metrics']['portfolio_count'] += 1
                        break

                if not assigned:
                    logger.warning(
                        f"No available node for portfolio {portfolio_id[:8]}... "
                        f"(all {len(healthy_nodes)} nodes at max capacity {self.MAX_PORTFOLIOS_PER_NODE})"
                    )

            # å‘é€è´Ÿè½½å‡è¡¡å®Œæˆé€šçŸ¥
            try:
                from ginkgo.notifier.core.notification_service import notify

                # è®¡ç®—è´Ÿè½½åˆ†å¸ƒ
                portfolio_distribution = {}
                for node_id in [n['node_id'] for n in healthy_nodes]:
                    count = sum(1 for pid in new_plan.values() if pid == node_id)
                    portfolio_distribution[node_id] = count

                notify(
                    f"è´Ÿè½½å‡è¡¡å®Œæˆ - {len(orphaned_portfolios)}ä¸ªPortfolioå·²é‡æ–°åˆ†é…",
                    level="INFO",
                    module="Scheduler",
                    details={
                        "é‡æ–°åˆ†é…æ•°": len(orphaned_portfolios),
                        "å¯ç”¨èŠ‚ç‚¹æ•°": len(healthy_nodes),
                        "è´Ÿè½½åˆ†å¸ƒ": str(portfolio_distribution)
                    }
                )
            except Exception as e:
                logger.warning(f"Failed to send load balancing notification: {e}")

        return new_plan

    def _get_plan_changes(
        self,
        old_plan: Dict[str, str],
        new_plan: Dict[str, str]
    ) -> Dict[str, tuple]:
        """
        æ¯”è¾ƒæ–°æ—§è°ƒåº¦è®¡åˆ’çš„å˜åŒ–

        Args:
            old_plan: æ—§è®¡åˆ’ {portfolio_id: node_id}
            new_plan: æ–°è®¡åˆ’ {portfolio_id: node_id}

        Returns:
            Dict: å˜åŒ–å­—å…¸ {portfolio_id: (old_node_id, new_node_id)}
                  old_node_id æˆ– new_node_id ä¸º None è¡¨ç¤ºæ–°å¢æˆ–åˆ é™¤
        """
        changes = {}

        # æ£€æŸ¥æ–°å¢å’Œå˜æ›´
        all_portfolio_ids = set(old_plan.keys()) | set(new_plan.keys())

        for portfolio_id in all_portfolio_ids:
            old_node = old_plan.get(portfolio_id)
            new_node = new_plan.get(portfolio_id)

            if old_node != new_node:
                changes[portfolio_id] = (old_node, new_node)

        return changes

    # ========================================================================
    # è°ƒåº¦æ›´æ–°å‘å¸ƒ
    # ========================================================================

    @time_logger(threshold=1.0)
    @retry(max_try=3)
    def _publish_schedule_update(
        self,
        old_plan: Dict[str, str],
        new_plan: Dict[str, str]
    ):
        """
        å‘å¸ƒè°ƒåº¦æ›´æ–°åˆ° Kafka

        æ¯”è¾ƒæ–°æ—§è®¡åˆ’ï¼Œåªå‘å¸ƒå˜æ›´çš„åˆ†é…ã€‚

        Args:
            old_plan: æ—§çš„è°ƒåº¦è®¡åˆ’ {portfolio_id: node_id}
            new_plan: æ–°çš„è°ƒåº¦è®¡åˆ’ {portfolio_id: node_id}
        """
        try:
            # æ‰¾å‡ºå˜æ›´
            changes = []
            for portfolio_id, new_node_id in new_plan.items():
                old_node_id = old_plan.get(portfolio_id)

                if old_node_id != new_node_id:
                    changes.append({
                        'portfolio_id': portfolio_id,
                        'from_node': old_node_id,  # None è¡¨ç¤ºæ–°åˆ†é…
                        'to_node': new_node_id,
                        'timestamp': datetime.now().isoformat()
                    })

            # å‘å¸ƒå˜æ›´åˆ° Kafka
            if changes:
                for change in changes:
                    self._send_schedule_command(change)

                # æ›´æ–° Redis ä¸­çš„è°ƒåº¦è®¡åˆ’
                self.redis_client.delete(self.SCHEDULE_PLAN_KEY)
                if new_plan:
                    self.redis_client.hset(self.SCHEDULE_PLAN_KEY, mapping=new_plan)

                # åªæ‰“å°æ€»ç»“æ—¥å¿—
                logger.info(f"Schedule updated: {len(changes)} portfolios assigned")

                # å‘é€è°ƒåº¦è®¡åˆ’å˜åŒ–é€šçŸ¥
                try:
                    from ginkgo.notifier.core.notification_service import notify

                    # ç»Ÿè®¡å˜åŒ–ç±»å‹
                    new_count = sum(1 for c in changes if c[1][0] in (None, "__ORPHANED__"))
                    migrate_count = sum(1 for c in changes if c[1][0] not in (None, "__ORPHANED__") and c[1][1] != "__ORPHANED__")
                    orphaned_count = sum(1 for c in changes if c[1][1] == "__ORPHANED__")

                    notify(
                        f"è°ƒåº¦è®¡åˆ’å·²æ›´æ–° - {len(changes)}ä¸ªPortfolioåˆ†é…å˜åŒ–",
                        level="INFO",
                        module="Scheduler",
                        details={
                            "æ€»å˜åŒ–æ•°": len(changes),
                            "æ–°åˆ†é…": new_count,
                            "è¿ç§»": migrate_count,
                            "å­¤å„¿": orphaned_count
                        }
                    )
                except Exception as e:
                    logger.warning(f"Failed to send schedule update notification: {e}")

        except Exception as e:
            logger.error(f"Failed to publish schedule update: {e}")

    def _send_schedule_command(self, change: Dict):
        """
        å‘é€è°ƒåº¦å‘½ä»¤åˆ° Kafka

        Args:
            change: å˜æ›´ä¿¡æ¯ {portfolio_id, from_node, to_node, timestamp}
        """
        try:
            # æ„é€ ExecutionNodeæœŸæœ›çš„æ¶ˆæ¯æ ¼å¼
            command = "portfolio.migrate"
            command_data = {
                "command": command,
                "portfolio_id": change['portfolio_id'],
                "source_node": change['from_node'],
                "target_node": change['to_node'],
                "timestamp": change['timestamp']
            }

            # [DEBUG] æ‰“å°Kafkaæ¶ˆæ¯
            logger.info(f"[KAFKA] Sending schedule command:")
            logger.info(f"  Topic: schedule.updates")
            logger.info(f"  Command: {command}")
            logger.info(f"  Portfolio: {change['portfolio_id'][:8]}...")
            logger.info(f"  Source: {change['from_node']}")
            logger.info(f"  Target: {change['to_node']}")

            # å‘é€åˆ°Kafka
            success = self.kafka_producer.send(
                topic="schedule.updates",
                msg=command_data
            )

            if not success:
                logger.error(f"[KAFKA] Failed to send portfolio {change['portfolio_id'][:8]} to Kafka")
            else:
                logger.info(f"[KAFKA] âœ“ Message sent successfully")

        except Exception as e:
            logger.error(f"Failed to send schedule command: {e}")

    # ========================================================================
    # å‘½ä»¤å¤„ç†ï¼ˆé€šè¿‡ Kafka æ¥æ”¶ä¸»åŠ¨è°ƒåº¦å‘½ä»¤ï¼‰
    # ========================================================================

    def _check_commands(self):
        """
        éé˜»å¡æ£€æŸ¥å¹¶å¤„ç†å‘½ä»¤

        è®¾è®¡è¦ç‚¹ï¼š
        - åœ¨ä¸»çº¿ç¨‹ä¸­è°ƒç”¨ï¼Œé¿å…çº¿ç¨‹åµŒå¥—
        - éé˜»å¡pollï¼Œä¸€æ¬¡å¤„ç†æ‰€æœ‰å¯ç”¨å‘½ä»¤
        - è°ƒç”¨æ—¶æœºï¼šæ¯æ¬¡è°ƒåº¦å¾ªç¯å + æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
        """
        if not self.command_consumer:
            return

        try:
            import json

            # éé˜»å¡åœ°æ£€æŸ¥å‘½ä»¤ï¼ˆtimeout_ms=0ï¼‰
            # TODO: é€‚é… GinkgoConsumer çš„æ¥å£
            # message = self.command_consumer.poll(timeout_ms=0)
            # while message:
            #     command_data = json.loads(message.value.decode('utf-8'))
            #     self._process_command(command_data)
            #     message = self.command_consumer.poll(timeout_ms=0)

            pass  # TODO: å®ç°å®é™…Kafkaæ¶ˆè´¹

        except Exception as e:
            logger.error(f"Error checking commands: {e}")

    def _process_command(self, command_data: Dict):
        """
        å¤„ç†å•ä¸ªå‘½ä»¤

        Args:
            command_data: å‘½ä»¤æ•°æ® {command, timestamp, params}
        """
        try:
            command = command_data.get('command')
            timestamp = command_data.get('timestamp')
            params = command_data.get('params', {})

            logger.info(f"Received command: {command} at {timestamp}")

            if command == 'recalculate':
                self._handle_recalculate(params)

            elif command == 'schedule':
                self._handle_schedule(params)

            elif command == 'migrate':
                self._handle_migrate(params)

            elif command == 'pause':
                self._handle_pause(params)

            elif command == 'resume':
                self._handle_resume(params)

            elif command == 'status':
                self._handle_status(params)

            else:
                logger.warning(f"Unknown command: {command}")

        except Exception as e:
            logger.error(f"Failed to process command: {e}")

    def _handle_recalculate(self, params: Dict):
        """
        å¤„ç†é‡æ–°è®¡ç®—å‘½ä»¤ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰

        Args:
            params: å‘½ä»¤å‚æ•° {force: bool, dry_run: bool}
        """
        force = params.get('force', False)

        # æ£€æŸ¥æš‚åœçŠ¶æ€
        if self.is_paused and not force:
            logger.warning("Scheduler is PAUSED, recalculate command rejected")
            logger.info("Use --force to override and execute recalculate while paused")
            return

        if self.is_paused and force:
            logger.warning("Scheduler is PAUSED, executing recalculate with --force")

        logger.info("Executing recalculate command")

        # ç«‹å³æ‰§è¡Œä¸€æ¬¡è°ƒåº¦å¾ªç¯
        self._schedule_loop()

        logger.info("Recalculate completed")

    def _handle_schedule(self, params: Dict):
        """
        å¤„ç†ç«‹å³è°ƒåº¦å‘½ä»¤

        Args:
            params: å‘½ä»¤å‚æ•° {force: bool}
        """
        force = params.get('force', False)

        # æ£€æŸ¥æš‚åœçŠ¶æ€
        if self.is_paused and not force:
            logger.warning("Scheduler is PAUSED, schedule command rejected")
            logger.info("Use --force to override and execute schedule while paused")
            return

        if self.is_paused and force:
            logger.warning("Scheduler is PAUSED, executing schedule with --force")

        logger.info("Executing immediate schedule command")

        # è·å–å½“å‰è°ƒåº¦è®¡åˆ’
        current_plan = self._get_current_schedule_plan()

        # è·å–æ‰€æœ‰ Portfolio
        all_portfolios = self._get_all_portfolios()

        # æ‰¾å‡ºæœªåˆ†é…çš„ Portfolio
        assigned_ids = set(current_plan.keys())
        unassigned = [p for p in all_portfolios if p['uuid'] not in assigned_ids]

        if unassigned:
            logger.info(f"Found {len(unassigned)} unassigned portfolios")

            # è·å–å¥åº·èŠ‚ç‚¹
            healthy_nodes = self._get_healthy_nodes()

            # åˆ†é…æœªåˆ†é…çš„ Portfolio
            new_plan = self._assign_portfolios(
                healthy_nodes=healthy_nodes,
                current_plan=current_plan,
                orphaned_portfolios=[p['uuid'] for p in unassigned]
            )

            # å‘å¸ƒè°ƒåº¦æ›´æ–°
            if new_plan != current_plan:
                self._publish_schedule_update(current_plan, new_plan)
                logger.info(f"Assigned {len(unassigned)} portfolios")
        else:
            logger.info("No unassigned portfolios to schedule")

    def _handle_migrate(self, params: Dict):
        """
        å¤„ç†è¿ç§»å‘½ä»¤

        Args:
            params: {portfolio_id, from_node, to_node}
        """
        portfolio_id = params.get('portfolio_id')
        from_node = params.get('from_node')
        to_node = params.get('to_node')

        logger.info(f"Migrating {portfolio_id} from {from_node} to {to_node}")

        # å‘å¸ƒè¿ç§»å‘½ä»¤åˆ° Kafka
        migration_command = {
            'command': 'portfolio.migrate',
            'portfolio_id': portfolio_id,
            'source_node': from_node,
            'target_node': to_node,
            'timestamp': datetime.now().isoformat()
        }

        # TODO: å‘é€åˆ° Kafka schedule.updates
        self._send_schedule_command({
            'portfolio_id': portfolio_id,
            'from_node': from_node,
            'to_node': to_node,
            'timestamp': migration_command['timestamp']
        })

        logger.info(f"Migration command sent for {portfolio_id}")

    def _handle_pause(self, params: Dict):
        """
        å¤„ç†æš‚åœå‘½ä»¤

        Args:
            params: {} (æ— å‚æ•°)
        """
        if self.is_paused:
            logger.info("Scheduler is already paused")
        else:
            self.is_paused = True
            logger.info(f"Scheduler {self.node_id} PAUSED - scheduling loop suspended")

    def _handle_resume(self, params: Dict):
        """
        å¤„ç†æ¢å¤å‘½ä»¤

        Args:
            params: {} (æ— å‚æ•°)
        """
        if not self.is_paused:
            logger.info("Scheduler is not paused")
        else:
            self.is_paused = False
            logger.info(f"Scheduler {self.node_id} RESUMED - scheduling loop restored")

    def _handle_status(self, params: Dict):
        """
        å¤„ç†çŠ¶æ€æŸ¥è¯¢å‘½ä»¤

        Args:
            params: {} (æ— å‚æ•°)
        """
        # ç¡®å®šä¸»çŠ¶æ€
        if self.should_stop:
            main_status = 'STOPPED'
        elif self.is_paused:
            main_status = 'PAUSED'
        elif self.is_running:
            main_status = 'RUNNING'
        else:
            main_status = 'INITIALIZED'

        # ç¡®å®šå‘½ä»¤å¯ç”¨æ€§
        if self.is_paused:
            commands_status = {
                'pause': 'already_paused',
                'resume': 'available',
                'recalculate': 'use --force',
                'schedule': 'use --force',
                'migrate': 'available',  # ç´§æ€¥å¹²é¢„å§‹ç»ˆå¯ç”¨
                'reload': 'available',   # é…ç½®é‡è½½å§‹ç»ˆå¯ç”¨
                'status': 'available'
            }
        else:
            commands_status = {
                'pause': 'available',
                'resume': 'not_paused',
                'recalculate': 'available',
                'schedule': 'available',
                'migrate': 'available',
                'reload': 'available',
                'status': 'available'
            }

        status = {
            'node_id': self.node_id,
            'main_status': main_status,
            'is_running': self.is_running,
            'is_paused': self.is_paused,
            'should_stop': self.should_stop,
            'schedule_interval': self.schedule_interval,
            'auto_scheduling': not self.is_paused and self.is_running,
            'commands_available': commands_status
        }

        logger.info(f"Scheduler status: {status}")

        # TODO: å¯ä»¥é€šè¿‡ Kafka æˆ–å…¶ä»–æ–¹å¼è¿”å›çŠ¶æ€ç»™è°ƒç”¨è€…
        # ä¾‹å¦‚ï¼šå‘é€åˆ° status.report topic æˆ– Redis

    def _send_status_report(self):
        """
        å‘é€è°ƒåº¦å™¨çŠ¶æ€æ±‡æŠ¥ï¼ˆæ¯30åˆ†é’Ÿï¼‰

        æ±‡æŠ¥å†…å®¹åŒ…æ‹¬ï¼š
        - å­˜æ´»èŠ‚ç‚¹åˆ—è¡¨
        - æ¯ä¸ªèŠ‚ç‚¹ä¸‹çš„Portfolioåˆ†é…æƒ…å†µ
        - æ€»ä½“è¿è¡ŒçŠ¶æ€
        """
        try:
            # è·å–å­˜æ´»èŠ‚ç‚¹
            healthy_nodes = self._get_healthy_nodes()

            # è·å–å½“å‰è°ƒåº¦è®¡åˆ’
            current_plan = self._get_current_schedule_plan()

            # æ„å»ºèŠ‚ç‚¹åˆ°Portfolioçš„æ˜ å°„
            node_portfolios = {}
            for node_id in [n['node_id'] for n in healthy_nodes]:
                portfolios_on_node = [
                    pid for pid, assigned_node in current_plan.items()
                    if assigned_node == node_id
                ]
                node_portfolios[node_id] = portfolios_on_node

            # æ„å»ºæ±‡æŠ¥è¯¦æƒ…
            node_details = []
            total_portfolios = 0

            for node in healthy_nodes:
                node_id = node['node_id']
                portfolio_count = len(node_portfolios.get(node_id, []))
                total_portfolios += portfolio_count
                metrics = node['metrics']

                node_details.append(
                    f"{node_id}: {portfolio_count}ä¸ªPortfolio "
                    f"(é˜Ÿåˆ—:{metrics['queue_size']}, CPU:{metrics['cpu_usage']:.1f}%)"
                )

            # å‘é€çŠ¶æ€æ±‡æŠ¥é€šçŸ¥
            from ginkgo.notifier.core.notification_service import notify

            notify(
                f"è°ƒåº¦å™¨çŠ¶æ€æ±‡æŠ¥ - {len(healthy_nodes)}ä¸ªå­˜æ´»èŠ‚ç‚¹, {total_portfolios}ä¸ªPortfolioè¿è¡Œä¸­",
                level="INFO",
                module="Scheduler",
                details={
                    "å­˜æ´»èŠ‚ç‚¹æ•°": len(healthy_nodes),
                    "è¿è¡ŒPortfolioæ€»æ•°": total_portfolios,
                    "èŠ‚ç‚¹è¯¦æƒ…": " | ".join(node_details[:5])  # é™åˆ¶é•¿åº¦
                }
            )

            logger.info(
                f"Status report sent: {len(healthy_nodes)} nodes, "
                f"{total_portfolios} portfolios"
            )

        except Exception as e:
            logger.warning(f"Failed to send status report: {e}")

    def _get_all_portfolios(self) -> List[Dict]:
        """
        è·å–æ‰€æœ‰ Portfolio

        Returns:
            List[Dict]: Portfolio åˆ—è¡¨
        """
        try:
            from ginkgo import services

            portfolio_service = services.data.portfolio_service()
            result = portfolio_service.get(is_live=True)

            if result.success:
                return result.data
            else:
                logger.error(f"Failed to get portfolios: {result.message}")
                return []

        except Exception as e:
            logger.error(f"Failed to get portfolios: {e}")
            return []

    def _discover_new_portfolios(self, current_plan: Dict[str, str]) -> List[str]:
        """
        å‘ç°æ–°çš„ live portfolioï¼ˆä»æ•°æ®åº“ä¸­æŸ¥æ‰¾ is_live=True ä½†ä¸åœ¨è°ƒåº¦è®¡åˆ’ä¸­çš„ï¼‰

        Args:
            current_plan: å½“å‰è°ƒåº¦è®¡åˆ’ {portfolio_id: node_id}

        Returns:
            List[str]: éœ€è¦åˆ†é…çš„ portfolio_id åˆ—è¡¨
        """
        try:
            # è·å–æ‰€æœ‰ live portfolioï¼ˆè¿”å›MPortfolioå¯¹è±¡åˆ—è¡¨ï¼‰
            all_portfolios = self._get_all_portfolios()

            if not all_portfolios:
                return []

            # æ‰¾å‡ºä¸åœ¨å½“å‰è®¡åˆ’ä¸­çš„ portfolio
            assigned_ids = set(current_plan.keys())
            new_portfolios = [p.uuid for p in all_portfolios if p.uuid not in assigned_ids]

            # åªåœ¨å‘ç°æ–°portfolioæ—¶æ‰“å°
            if new_portfolios:
                logger.info(f"New portfolios: {[p[:8] for p in new_portfolios]}")

            return new_portfolios

        except Exception as e:
            logger.error(f"Failed to discover new portfolios: {e}")
            return []
