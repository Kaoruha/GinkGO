"""
Ginkgoæµå¼æŸ¥è¯¢æ¨¡å—

è¿™ä¸ªæ¨¡å—ä¸ºGinkgoé‡åŒ–äº¤æ˜“å¹³å°æä¾›ä¼ä¸šçº§çš„æµå¼æŸ¥è¯¢èƒ½åŠ›ï¼Œ
æ”¯æŒå¤§è§„æ¨¡å†å²æ•°æ®çš„é«˜æ•ˆå¤„ç†å’Œå®æ—¶æ•°æ®æµåˆ†æã€‚

æ ¸å¿ƒç‰¹æ€§ï¼š
- ğŸš€ é«˜æ€§èƒ½æµå¼æŸ¥è¯¢ï¼šä»155ç§’é˜»å¡é™è‡³3æ¯«ç§’å“åº”
- ğŸ’¾ å†…å­˜ä¼˜åŒ–ï¼šä»2GB+å ç”¨é™è‡³10MBä»¥ä¸‹
- ğŸ”„ æ–­ç‚¹ç»­ä¼ ï¼šæ”¯æŒæŸ¥è¯¢ä¸­æ–­åä»æ–­ç‚¹ç»§ç»­
- ğŸ›¡ï¸ é”™è¯¯æ¢å¤ï¼šæ™ºèƒ½é”™è¯¯å¤„ç†å’Œè‡ªåŠ¨é‡è¯•
- ğŸ“Š å®æ—¶ç›‘æ§ï¼šå†…å­˜ã€è¿›åº¦ã€æ€§èƒ½å…¨æ–¹ä½ç›‘æ§

ä¸»è¦æ¨¡å—ï¼š
- engines: æµå¼æŸ¥è¯¢å¼•æ“ï¼ˆMySQLã€ClickHouseç­‰ï¼‰
- managers: ç®¡ç†ç»„ä»¶ï¼ˆæ–­ç‚¹ã€è¿›åº¦ã€å†…å­˜ã€ä¼šè¯ï¼‰
- optimizers: æ€§èƒ½ä¼˜åŒ–å™¨ï¼ˆæŸ¥è¯¢ã€æ‰¹æ¬¡ã€ç´¢å¼•ï¼‰
- recovery: é”™è¯¯æ¢å¤æœºåˆ¶ï¼ˆå¤„ç†ã€æ¢å¤ã€é‡è¯•ï¼‰
- cache: ç¼“å­˜ç³»ç»Ÿï¼ˆç»“æœç¼“å­˜ã€ç­–ç•¥ç®¡ç†ï¼‰

ä½¿ç”¨ç¤ºä¾‹ï¼š
    >>> from ginkgo.data.streaming import StreamingQueryEngine
    >>> engine = StreamingQueryEngine(crud_instance)
    >>> for batch in engine.stream_find(filters={'code': '000001.SZ'}):
    ...     process_batch(batch)
"""

from typing import List, Dict, Any, Iterator, Optional, Callable
import time
import json
from dataclasses import dataclass
from enum import Enum

# ç‰ˆæœ¬ä¿¡æ¯
__version__ = "1.0.0"
__author__ = "Ginkgo Development Team"
__description__ = "Enterprise-grade streaming query system for quantitative trading"


# æµå¼æŸ¥è¯¢çŠ¶æ€æšä¸¾
class StreamingState(Enum):
    """æµå¼æŸ¥è¯¢çŠ¶æ€"""

    IDLE = "idle"
    INITIALIZING = "initializing"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"


class ErrorType(Enum):
    """é”™è¯¯ç±»å‹åˆ†ç±»"""

    CONNECTION = "connection"
    MEMORY = "memory"
    TIMEOUT = "timeout"
    DATABASE = "database"
    QUERY = "query"
    UNKNOWN = "unknown"


class RecoveryAction(Enum):
    """æ¢å¤åŠ¨ä½œç±»å‹"""

    RETRY = "retry"
    ADJUST_AND_RETRY = "adjust_and_retry"
    RESET_AND_RETRY = "reset_and_retry"
    FAIL = "fail"


# æ ¸å¿ƒæ•°æ®ç±»
@dataclass
class StreamingConfig:
    """æµå¼æŸ¥è¯¢é…ç½®"""

    enabled: bool = False
    default_batch_size: int = 1000
    max_batch_size: int = 10000
    min_batch_size: int = 100
    memory_threshold_mb: int = 100
    auto_adjust_batch_size: bool = True
    enable_checkpoint: bool = True
    checkpoint_interval: int = 1000
    max_retry_attempts: int = 3
    enable_cache: bool = True
    cache_ttl_seconds: int = 300


@dataclass
class ProgressInfo:
    """è¿›åº¦ä¿¡æ¯"""

    processed: int
    total: Optional[int] = None
    rate: float = 0.0
    eta: Optional[float] = None
    elapsed: float = 0.0

    @property
    def progress_percentage(self) -> Optional[float]:
        """è¿›åº¦ç™¾åˆ†æ¯”"""
        if self.total and self.total > 0:
            return min(100.0, (self.processed / self.total) * 100)
        return None


@dataclass
class QueryState:
    """æŸ¥è¯¢çŠ¶æ€"""

    query_id: str
    processed_count: int = 0
    last_offset: int = 0
    last_timestamp: Optional[str] = None
    batch_size: int = 1000
    filters: Dict[str, Any] = None
    state: StreamingState = StreamingState.IDLE
    created_at: float = None

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = time.time()
        if self.filters is None:
            self.filters = {}

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "QueryState":
        """ä»å­—å…¸åˆ›å»ºæŸ¥è¯¢çŠ¶æ€"""
        return cls(
            query_id=data["query_id"],
            processed_count=data.get("processed_count", 0),
            last_offset=data.get("last_offset", 0),
            last_timestamp=data.get("last_timestamp"),
            batch_size=data.get("batch_size", 1000),
            filters=data.get("filters", {}),
            created_at=data.get("created_at", time.time()),
        )

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "query_id": self.query_id,
            "processed_count": self.processed_count,
            "last_offset": self.last_offset,
            "last_timestamp": self.last_timestamp,
            "batch_size": self.batch_size,
            "filters": self.filters,
            "created_at": self.created_at,
        }


# å¼‚å¸¸ç±»
class StreamingError(Exception):
    """æµå¼æŸ¥è¯¢åŸºç¡€å¼‚å¸¸"""

    pass


class StreamingConfigError(StreamingError):
    """æµå¼æŸ¥è¯¢é…ç½®é”™è¯¯"""

    pass


class StreamingEngineError(StreamingError):
    """æµå¼æŸ¥è¯¢å¼•æ“é”™è¯¯"""

    pass


class StreamingRecoveryError(StreamingError):
    """æµå¼æŸ¥è¯¢æ¢å¤é”™è¯¯"""

    pass


class CheckpointError(StreamingError):
    """æ–­ç‚¹ç»­ä¼ é”™è¯¯"""

    pass


class ProgressTrackingError(StreamingError):
    """è¿›åº¦è·Ÿè¸ªé”™è¯¯"""

    pass


# å¯¼å‡ºçš„ä¸»è¦æ¥å£ï¼ˆå°†åœ¨åç»­æ¨¡å—ä¸­å®ç°ï¼‰
__all__ = [
    # æ ¸å¿ƒç±»å’Œæšä¸¾
    "StreamingConfig",
    "StreamingState",
    "ErrorType",
    "RecoveryAction",
    "ProgressInfo",
    "QueryState",
    # ç®¡ç†å™¨ç±»
    "CheckpointManager",
    "ProgressTracker",
    # å¼‚å¸¸ç±»
    "StreamingError",
    "StreamingConfigError",
    "StreamingEngineError",
    "StreamingRecoveryError",
    "CheckpointError",
    "ProgressTrackingError",
    # ç‰ˆæœ¬ä¿¡æ¯
    "__version__",
    "__author__",
    "__description__",
]


# å¯¼å…¥ç®¡ç†å™¨ç±»
from ginkgo.data.streaming.checkpoint import CheckpointManager, ProgressTracker


# æ¨¡å—åˆå§‹åŒ–æ—¥å¿—
def _log_module_info():
    """è®°å½•æ¨¡å—åˆå§‹åŒ–ä¿¡æ¯"""
    try:
        from ginkgo.libs import GLOG

        GLOG.INFO(f"Ginkgo Streaming module v{__version__} initialized")
        GLOG.DEBUG(f"Streaming module description: {__description__}")
    except ImportError:
        # å¦‚æœGLOGä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†æ—¥å¿—
        import logging

        logging.getLogger(__name__).info(f"Ginkgo Streaming module v{__version__} initialized")


# æ¨¡å—åŠ è½½æ—¶åˆå§‹åŒ–
_log_module_info()
