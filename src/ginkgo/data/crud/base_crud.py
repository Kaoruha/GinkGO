from typing import TypeVar, Generic, List, Optional, Any, Union, Dict, Callable, Type
from abc import ABC, abstractmethod
import pandas as pd
from decimal import Decimal
from datetime import datetime
from sqlalchemy import and_, delete, select, text, update
from sqlalchemy.orm import Session

from ginkgo.data.drivers import get_db_connection, add, add_all
from ginkgo.data.models import MClickBase, MMysqlBase
from ginkgo.libs import GLOG, time_logger, retry, cache_with_expiration
from ginkgo.data.access_control import restrict_crud_access
from ginkgo.data.crud.model_crud_mapping import ModelCRUDMapping
from ginkgo.data.crud.model_conversion import ModelList

T = TypeVar("T", bound=Union[MClickBase, MMysqlBase])


class CRUDResult:
    """
    CRUDæŸ¥è¯¢ç»“æžœåŒ…è£…å™¨

    ç»Ÿä¸€å¤„ç†CRUDæ“ä½œçš„ç»“æžœï¼Œæ”¯æŒé“¾å¼è½¬æ¢ï¼š
    - to_entities(): è½¬æ¢ä¸ºå®žä½“
    - to_dataframe(): è½¬æ¢ä¸ºDataFrame
    - first(): èŽ·å–ç¬¬ä¸€ä¸ªç»“æžœ
    - count(): èŽ·å–ç»“æžœæ•°é‡
    """

    def __init__(self, models: List[T], crud_instance: 'BaseCRUD'):
        """
        åˆå§‹åŒ–CRUDç»“æžœåŒ…è£…å™¨

        Args:
            models: åŽŸå§‹æ¨¡åž‹å¯¹è±¡åˆ—è¡¨
            crud_instance: CRUDå®žä¾‹ï¼Œç”¨äºŽè½¬æ¢æ–¹æ³•
        """
        self._models = models
        self._crud_instance = crud_instance
        self._business_objects_cache = None
        self._dataframe_cache = None

    def to_business_objects(self) -> List[Any]:
        """
        è½¬æ¢ä¸ºä¸šåŠ¡å¯¹è±¡

        Returns:
            List of business objects with enum fields converted
        """
        if self._business_objects_cache is None:
            self._business_objects_cache = self._crud_instance._convert_to_business_objects(self._models)
        return self._business_objects_cache

    def to_dataframe(self) -> pd.DataFrame:
        """
        è½¬æ¢ä¸ºDataFrame

        Returns:
            pandas DataFrame containing the data
        """
        if self._dataframe_cache is None:
            if not self._models:
                self._dataframe_cache = pd.DataFrame()
            else:
                # å…ˆå°†æ¨¡åž‹åˆ—è¡¨è½¬æ¢ä¸ºDataFrame
                df = pd.DataFrame([model.__dict__ for model in self._models])
                # ç§»é™¤éžæ•°æ®åˆ—
                columns_to_remove = ['_sa_instance_state']
                for col in columns_to_remove:
                    if col in df.columns:
                        df = df.drop(col, axis=1)
                # åº”ç”¨enumè½¬æ¢
                self._dataframe_cache = self._crud_instance._process_dataframe_output(df)
        return self._dataframe_cache

    def first(self) -> Optional[T]:
        """
        èŽ·å–ç¬¬ä¸€ä¸ªç»“æžœ

        Returns:
            First model in the result list, or None if empty
        """
        return self._models[0] if self._models else None

    def count(self) -> int:
        """
        èŽ·å–ç»“æžœæ•°é‡

        Returns:
            Number of results
        """
        return len(self._models)

    def is_empty(self) -> bool:
        """
        æ£€æŸ¥ç»“æžœæ˜¯å¦ä¸ºç©º

        Returns:
            True if no results, False otherwise
        """
        return len(self._models) == 0

    def filter(self, predicate: Callable[[T], bool]) -> 'CRUDResult':
        """
        è¿‡æ»¤ç»“æžœ

        Args:
            predicate: è¿‡æ»¤å‡½æ•°ï¼ŒæŽ¥å—modelå‚æ•°ï¼Œè¿”å›žbool

        Returns:
            New CRUDResult with filtered models
        """
        filtered_models = [model for model in self._models if predicate(model)]
        return CRUDResult(filtered_models, self._crud_instance)

    def map(self, func: Callable[[T], Any]) -> List[Any]:
        """
        æ˜ å°„ç»“æžœ

        Args:
            func: æ˜ å°„å‡½æ•°ï¼ŒæŽ¥å—modelå‚æ•°

        Returns:
            List of mapped results
        """
        return [func(model) for model in self._models]

    def __iter__(self):
        """æ”¯æŒè¿­ä»£"""
        return iter(self._models)

    def __len__(self):
        """æ”¯æŒlen()å‡½æ•°"""
        return len(self._models)

    def __getitem__(self, index):
        """æ”¯æŒç´¢å¼•è®¿é—®"""
        return self._models[index]

    def __bool__(self):
        """æ”¯æŒå¸ƒå°”åˆ¤æ–­"""
        return bool(self._models)

    def __repr__(self):
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        model_type = type(self._models[0]).__name__ if self._models else "No models"
        return f"CRUDResult(count={len(self._models)}, type={model_type})"


@restrict_crud_access
class BaseCRUD(Generic[T], ABC):
    """
    Generic base CRUD class with template method pattern for unified decorator management.

    Features:
    - Template methods with unified decorators (@time_logger, @retry, @cache_with_expiration)
    - Hook methods that subclasses can override without worrying about decorators
    - Type-safe operations with TypeVar
    - Automatic database detection (ClickHouse vs MySQL)
    - Database-specific operation handling (ClickHouse limitations)
    - Unified validation architecture via _get_field_config() hook method
    - Unified conversion interface with ModelConversion capabilities

    Validation Architecture:
    - create() â†’ _validate_before_database() â†’ validate_data_by_config(_get_field_config())
    - Subclasses define validation rules by overriding _get_field_config()
    - All validation is configuration-driven and executed automatically

    Subclass Requirements:
    - Must override _model_class with the appropriate Model class
    - Must implement _get_field_config() for validation
    """

    # æŠ½è±¡å±žæ€§ï¼šå­ç±»å¿…é¡»é‡å†™
    # ç”¨æ³•è¯´æ˜Žï¼š
    # 1. å­ç±»å¿…é¡»è®¾ç½® _model_class = MYourModel æ¥æŒ‡å®šå¯¹åº”çš„Modelç±»
    # 2. __init_subclass__ ä¼šéªŒè¯ _model_class æ˜¯å¦è¢«æ­£ç¡®è®¾ç½®
    # 3. è‡ªåŠ¨æ³¨å†Œæœºåˆ¶ä¼šä½¿ç”¨ _model_class æ¥å»ºç«‹ Model-CRUD æ˜ å°„å…³ç³»
    # 4. å¦‚æžœ _model_class = Noneï¼Œåˆ™æŠ›å‡º NotImplementedErrorï¼ˆç‰¹æ®Šç±»å¦‚TickCRUDé™¤å¤–ï¼‰
    _model_class: Optional[Type[T]] = None

    # ClickHouse string fields that need null byte cleaning (from external sources or user input)
    CLICKHOUSE_STRING_FIELDS = [
        "code",  # Stock code (from external data sources like Tushare, Yahoo)
        "name",  # Names (user-defined)
        "reason",  # Reasons (user input or strategy generated)
        "meta",  # Metadata (may contain external data)
        "desc",  # Descriptions (user input)
        "portfolio_id",  # Portfolio ID (may have FixedString padding)
    ]

    def __init__(self, model_class: type[T]):
        """
        åˆå§‹åŒ–CRUDå®žä¾‹

        Args:
            model_class: å¯¹åº”çš„Modelç±»ï¼Œå¦‚ MBar, MStockInfo ç­‰

        ç”¨æ³•è¯´æ˜Žï¼š
        1. model_class å‚æ•°ç”¨äºŽè®¾ç½®å®žä¾‹å˜é‡ self.model_class
        2. ä¸Žç±»çº§åˆ«çš„ _model_class ä¸åŒï¼š_model_class ç”¨äºŽæ³¨å†Œï¼Œmodel_class ç”¨äºŽè¿è¡Œæ—¶
        3. å­ç±»é€šå¸¸è°ƒç”¨ super().__init__(MYourModel) æ¥ä¼ é€’æ­£ç¡®çš„Modelç±»
        """
        self.model_class = model_class
        self._is_clickhouse = issubclass(model_class, MClickBase)
        self._is_mysql = issubclass(model_class, MMysqlBase)

        if not (self._is_clickhouse or self._is_mysql):
            raise ValueError(f"Model {model_class} must inherit from MClickBase or MMysqlBase")

    def __init_subclass__(cls, **kwargs):
        """
        ðŸŽ¯ å…³é”®ï¼šCRUDå­ç±»åˆ›å»ºæ—¶è‡ªåŠ¨æ³¨å†Œå…³ç³»å’ŒéªŒè¯
        å½“ä»»ä½•CRUDå­ç±»è¢«å®šä¹‰æ—¶ï¼Œè¿™ä¸ªæ–¹æ³•ä¼šè¢«Pythonè‡ªåŠ¨è°ƒç”¨

        _model_class ç”Ÿå‘½å‘¨æœŸå’Œä½œç”¨ï¼š
        1. ç±»å®šä¹‰æ—¶ï¼šPythonè°ƒç”¨ __init_subclass__ï¼ŒéªŒè¯ cls._model_class æ˜¯å¦è®¾ç½®
        2. éªŒè¯é˜¶æ®µï¼šæ£€æŸ¥ _model_class æ˜¯å¦ç»§æ‰¿è‡ª MClickBase æˆ– MMysqlBase
        3. æ³¨å†Œé˜¶æ®µï¼šä½¿ç”¨ _model_class è°ƒç”¨ ModelCRUDMapping.register(model_class, cls)
        4. è¿è¡Œæ—¶ï¼šå¯é€šè¿‡ self.model_class è®¿é—®å®žä¾‹å˜é‡ï¼ˆç”±__init__è®¾ç½®ï¼‰

        ç‰¹æ®Šæƒ…å†µï¼š
        - TickCRUD ç­‰åŠ¨æ€Modelç±»å¯ä»¥è±å… _model_class éªŒè¯
        - å…¶ä»–æ‰€æœ‰CRUDå­ç±»å¿…é¡»è®¾ç½® _model_class
        """
        super().__init_subclass__(**kwargs)

        # éªŒè¯å­ç±»æ˜¯å¦é‡å†™äº† _model_classï¼ˆç‰¹æ®Šæƒ…å†µå¦‚TickCRUDå¯ä»¥è±å…ï¼‰
        if cls._model_class is None and cls.__name__ not in ['TickCRUD']:
            raise NotImplementedError(
                f"CRUD subclass '{cls.__name__}' must override '_model_class' attribute. "
                f"Example: _model_class = MYourModel"
            )

        # éªŒè¯ _model_class æ˜¯å¦ç»§æ‰¿è‡ªæ­£ç¡®çš„åŸºç±»ï¼ˆç‰¹æ®Šæƒ…å†µè·³è¿‡éªŒè¯ï¼‰
        if cls.__name__ not in ['TickCRUD']:
            model_class = cls._model_class
            if not (issubclass(model_class, MClickBase) or issubclass(model_class, MMysqlBase)):
                raise TypeError(
                    f"CRUD subclass '{cls.__name__}': _model_class must inherit from "
                    f"MClickBase or MMysqlBase, but got {model_class.__bases__}"
                )

            # è‡ªåŠ¨å»ºç«‹Model-CRUDæ˜ å°„å…³ç³»
            # è¿™é‡Œä½¿ç”¨ä»Žç±»çº§åˆ« _model_class èŽ·å–çš„ model_class è¿›è¡Œæ³¨å†Œ
            # å»ºç«‹æ˜ å°„åŽï¼Œå¯ä»¥é€šè¿‡ ModelCRUDMapping.get_crud_class(MBar) æ‰¾åˆ° BarCRUD
            ModelCRUDMapping.register(model_class, cls)
            GLOG.DEBUG(f"Auto-registered: {model_class.__name__} â†’ {cls.__name__}")
        else:
            GLOG.DEBUG(f"Skipped auto-registration for {cls.__name__} (dynamic model class)")

    def _get_connection(self):
        """Get appropriate database connection based on model type"""
        return get_db_connection(self.model_class)

    def get_session(self) -> Session:
        """Get a new database session for external transaction management."""
        return self._get_connection().get_session()

    def _clean_clickhouse_strings(self, data):
        """
        Clean ClickHouse FixedString null bytes for configured string fields.

        Args:
            data: Can be a list of values, pandas DataFrame, or single value

        Returns:
            Cleaned data in the same format as input
        """
        if not self._is_clickhouse:
            return data

        # Handle list of string values (for DISTINCT queries)
        if isinstance(data, list):
            return [str(value).strip("\x00") if isinstance(value, str) else value for value in data]

        # Handle pandas DataFrame
        elif isinstance(data, pd.DataFrame) and data.shape[0] > 0:
            df = data.copy()
            for field in self.CLICKHOUSE_STRING_FIELDS:
                if field in df.columns:
                    df[field] = df[field].astype(str).str.replace("\x00", "", regex=False)
            return df

        # Handle single string value
        elif isinstance(data, str):
            return data.strip("\x00")

        return data

    # ============================================================================
    # é…ç½®åŒ–æ•°æ®éªŒè¯ - ç®€æ´ç»Ÿä¸€çš„éªŒè¯æ–¹æ³•
    # ============================================================================

    def _validate_before_database(self, data: dict) -> dict:
        """
        ä¸¤å±‚é…ç½®åŒ–æ•°æ®éªŒè¯ï¼šæ•°æ®åº“å¿…å¡«å­—æ®µ + ä¸šåŠ¡ç‰¹å®šå­—æ®µ

        Args:
            data: å¾…éªŒè¯çš„æ•°æ®å­—å…¸

        Returns:
            éªŒè¯å¹¶è½¬æ¢åŽçš„æ•°æ®å­—å…¸

        Raises:
            ValidationError: å½“éªŒè¯å¤±è´¥æ—¶
        """
        from ginkgo.data.crud.validation import validate_data_by_config, ValidationError

        try:
            validated = data.copy()

            # ç¬¬ä¸€å±‚ï¼šæ•°æ®åº“å¿…å¡«å­—æ®µéªŒè¯
            database_required_config = self._get_database_required_config()
            if database_required_config:
                validated = validate_data_by_config(validated, database_required_config)
                GLOG.DEBUG(f"Database required fields validation passed for {self.model_class.__name__}")

            # ç¬¬äºŒå±‚ï¼šä¸šåŠ¡ç‰¹å®šå­—æ®µéªŒè¯
            business_field_config = self._get_field_config()
            if business_field_config:
                validated = validate_data_by_config(validated, business_field_config)
                GLOG.DEBUG(f"Business fields validation passed for {self.model_class.__name__}")

            # å¦‚æžœä¸¤å±‚éƒ½æ²¡æœ‰é…ç½®
            if not database_required_config and not business_field_config:
                GLOG.DEBUG(f"No validation config defined for {self.model_class.__name__}, skipping validation")

            GLOG.DEBUG(f"Complete data validation passed for {self.model_class.__name__}")
            return validated

        except ValidationError as e:
            GLOG.ERROR(f"Data validation failed for {self.model_class.__name__}: {e}")
            raise e
        except Exception as e:
            GLOG.ERROR(f"Unexpected error during validation for {self.model_class.__name__}: {e}")
            raise ValidationError(f"Unexpected validation error: {str(e)}")

    def _get_database_required_config(self) -> dict:
        """
        èŽ·å–æ•°æ®åº“å¿…å¡«å­—æ®µé…ç½® - åŸºäºŽæ¨¡åž‹åŸºç±»å®šä¹‰æ·»åŠ æ•°æ®æ—¶å¿…é¡»ä¼ å…¥çš„å­—æ®µ

        Returns:
            dict: æ•°æ®åº“å¿…å¡«å­—æ®µé…ç½®
        """
        if self._is_mysql:
            return self._get_mysql_required_config()
        elif self._is_clickhouse:
            return self._get_clickhouse_required_config()
        return {}

    def _get_mysql_required_config(self) -> dict:
        """
        MySQL å¿…å¡«å­—æ®µé…ç½® - åŸºäºŽ MMysqlBase

        MMysqlBase çš„æ‰€æœ‰å­—æ®µéƒ½æœ‰é»˜è®¤å€¼ï¼Œå› æ­¤æ— å¿…å¡«å­—æ®µï¼š
        - uuid: è‡ªåŠ¨ç”Ÿæˆ
        - meta: é»˜è®¤ "{}"
        - desc: é»˜è®¤æè¿°æ–‡æœ¬
        - create_at: è‡ªåŠ¨ç”Ÿæˆå½“å‰æ—¶é—´
        - update_at: è‡ªåŠ¨ç”Ÿæˆå½“å‰æ—¶é—´
        - is_del: é»˜è®¤ False
        - source: é»˜è®¤ OTHER

        Returns:
            dict: MySQL å¿…å¡«å­—æ®µé…ç½®ï¼ˆé€šå¸¸ä¸ºç©ºï¼‰
        """
        return {}

    def _get_clickhouse_required_config(self) -> dict:
        """
        ClickHouse å¿…å¡«å­—æ®µé…ç½® - åŸºäºŽ MClickBase

        MClickBase å¿…å¡«å­—æ®µï¼š
        - timestamp: æ²¡æœ‰é»˜è®¤å€¼ï¼Œå¿…é¡»ä¼ å…¥ï¼ˆMergeTree æŽ’åºé”®ï¼‰

        å…¶ä»–å­—æ®µéƒ½æœ‰é»˜è®¤å€¼ï¼š
        - uuid: è‡ªåŠ¨ç”Ÿæˆ
        - meta: é»˜è®¤ "{}"
        - desc: é»˜è®¤æè¿°æ–‡æœ¬
        - source: é»˜è®¤ OTHER

        Returns:
            dict: ClickHouse å¿…å¡«å­—æ®µé…ç½®
        """
        return {"timestamp": {"type": ["datetime", "string"]}}

    def _get_field_config(self) -> dict:
        """
        èŽ·å–ä¸šåŠ¡å­—æ®µé…ç½® - å­ç±»é‡å†™æ­¤æ–¹æ³•å®šä¹‰ä¸šåŠ¡å¿…å¡«å­—æ®µè¦æ±‚

        é…ç½®ä¸­çš„æ‰€æœ‰å­—æ®µéƒ½æ˜¯å¿…å¡«çš„ï¼Œæ”¯æŒçš„é…ç½®å‚æ•°ï¼š
        - type: å­—æ®µç±»åž‹ï¼Œå¯ä»¥æ˜¯å•ä¸ªç±»åž‹æˆ–ç±»åž‹åˆ—è¡¨
        - min/max: æ•°å€¼èŒƒå›´æˆ–å­—ç¬¦ä¸²é•¿åº¦èŒƒå›´
        - choices: æžšä¸¾å€¼åˆ—è¡¨
        - pattern: æ­£åˆ™è¡¨è¾¾å¼ï¼ˆç”¨äºŽå­—ç¬¦ä¸²ï¼‰

        Returns:
            dict: ä¸šåŠ¡å­—æ®µé…ç½®å­—å…¸ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
            {
                'field_name': {
                    'type': 'string' | ['int', 'string'],  # å•ç±»åž‹æˆ–å¤šç±»åž‹
                    'min': 0,                               # æœ€å°å€¼/é•¿åº¦
                    'max': 100,                             # æœ€å¤§å€¼/é•¿åº¦
                    'choices': [value1, value2],            # æžšä¸¾å€¼
                    'pattern': r'regex_pattern'             # æ­£åˆ™è¡¨è¾¾å¼
                },
                ...
            }

        Example:
            return {
                'code': {'type': 'string', 'pattern': r'^[0-9]{6}\\.(SZ|SH)$'},
                'price': {'type': ['decimal', 'float'], 'min': 0.001},
                'volume': {'type': ['int', 'string'], 'min': 0}
            }
        """
        return {}

    # ============================================================================
    # Template Methods - With unified decorators, should not be overridden
    # ============================================================================

    @time_logger
    @retry(max_try=3)
    def add(self, item: T, session: Optional[Session] = None) -> T:
        """
        Template method: Add single item to database.
        ç›´æŽ¥æ·»åŠ å¯¹è±¡ï¼Œä¸è¿›è¡Œæ•°æ®éªŒè¯ï¼Œä¾èµ–æ•°æ®åº“çº¦æŸç¡®ä¿æ•°æ®å®Œæ•´æ€§ã€‚
        Subclasses should override _do_add() instead.

        Args:
            item: Model instance to add
            session: Optional SQLAlchemy session to use for the operation.

        Returns:
            Added model instance
        """
        try:
            return self._do_add(item, session)
        except Exception as e:
            GLOG.ERROR(f"Failed to add {self.model_class.__name__} item: {e}")
            raise

    @time_logger
    @retry(max_try=3)
    def add_batch(self, items: List[Any], session: Optional[Session] = None) -> ModelList:
        """
        Template method: Add multiple items to database in batch.
        æ”¯æŒè‡ªåŠ¨ç±»åž‹è½¬æ¢ï¼Œä¸è¿›è¡Œæ•°æ®éªŒè¯ï¼Œä¾èµ–æ•°æ®åº“çº¦æŸç¡®ä¿æ•°æ®å®Œæ•´æ€§ã€‚
        Subclasses should override _do_add_batch() instead.

        Args:
            items: List of model instances or convertible objects
            session: Optional SQLAlchemy session to use for the operation.

        Returns:
            ModelList of added model instances with conversion capabilities
        """
        try:
            # åªè¿›è¡Œç±»åž‹è½¬æ¢ï¼Œä¸è¿›è¡Œæ•°æ®éªŒè¯
            converted_items = self._convert_input_batch(items)
            result = self._do_add_batch(converted_items, session)
            # è¿”å›žå®žé™…æ’å…¥çš„å¯¹è±¡ï¼ˆå·²è§£ç»‘sessionï¼‰ï¼Œè€Œä¸æ˜¯åŽŸå§‹è½¬æ¢çš„å¯¹è±¡
            return ModelList(converted_items, self)
        except Exception as e:
            GLOG.ERROR(f"Failed to add {self.model_class.__name__} items in batch: {e}")
            raise

    @time_logger
    @retry(max_try=3)
    def create(self, session: Optional[Session] = None, **kwargs) -> T:
        """
        Template method: Create object from parameters and add to database.
        Applies data validation before creating object.
        Subclasses should override _create_from_params() instead.

        Args:
            session: Optional SQLAlchemy session to use for the operation.
            **kwargs: Parameters to create the object

        Returns:
            Created model instance
        """
        try:
            # å…ˆéªŒè¯å‚æ•°æ•°æ®
            validated_kwargs = self._validate_before_database(kwargs)
            item = self._create_from_params(**validated_kwargs)
            return self._do_add(item, session)
        except Exception as e:
            GLOG.ERROR(f"Failed to create {self.model_class.__name__} from params: {e}")
            raise

    @time_logger
    def find(
        self,
        filters: Optional[Dict[str, Any]] = None,
        page: Optional[int] = None,
        page_size: Optional[int] = None,
        order_by: Optional[str] = None,
        desc_order: bool = False,
        as_dataframe: bool = False,
        distinct_field: Optional[str] = None,
        session: Optional[Session] = None,
    ) -> ModelList[T]:
        """
        Template method: Find items with enhanced filters and pagination.
        Supports operator filters like field__gte, field__lte, field__in.
        Subclasses should override _do_find() instead.

        Args:
            filters: Dictionary of field -> value filters (supports operators)
                    Examples: {"code": "000001.SZ", "timestamp__gte": "2023-01-01"}
            page: Page number (0-based)
            page_size: Number of items per page
            order_by: Field name to order by
            desc_order: Whether to use descending order
            as_dataframe: DEPRECATED - Use models.to_dataframe() instead
            distinct_field: Field name for DISTINCT query (returns unique values of this field)
            session: Optional SQLAlchemy session to use for the operation.

        Returns:
            ModelList[T] - æ”¯æŒto_dataframe()å’Œto_entities()æ–¹æ³•
        """
        try:
            # Execute query using existing _do_find method
            raw_results = self._do_find(
                filters, page, page_size, order_by, desc_order, False, "model", distinct_field, session
            )

            # Ensure we always return a list
            if isinstance(raw_results, list):
                models = raw_results
            else:
                models = [raw_results] if raw_results else []

            # Return ModelList with conversion capabilities
            return ModelList(models, self)

        except Exception as e:
            GLOG.ERROR(f"Failed to find {self.model_class.__name__} items: {e}")
            return ModelList([], self)

    def remove(self, filters: Dict[str, Any], session: Optional[Session] = None) -> None:
        """
        Template method: Remove items by filters.
        Subclasses should override _do_remove() instead.

        Args:
            filters: Dictionary of field -> value filters for deletion
            session: Optional SQLAlchemy session to use for the operation.
        """
        if not filters:
            GLOG.ERROR("Remove operation requires filters for safety")
            return

        try:
            return self._do_remove(filters, session)
        except Exception as e:
            GLOG.ERROR(f"Failed to remove {self.model_class.__name__} items: {e}")
            raise

    @time_logger
    def modify(self, filters: Dict[str, Any], updates: Dict[str, Any], session: Optional[Session] = None) -> None:
        """
        Template method: Update items by filters.
        Subclasses should override _do_modify() instead.

        Args:
            filters: Dictionary of field -> value filters for selection
            updates: Dictionary of field -> value updates to apply
            session: Optional SQLAlchemy session to use for the operation.
        """
        if not filters or not updates:
            GLOG.ERROR("Modify operation requires both filters and updates")
            return

        if self._is_clickhouse:
            GLOG.ERROR("ClickHouse doesn't support UPDATE operations")
            return

        try:
            return self._do_modify(filters, updates, session)
        except Exception as e:
            GLOG.ERROR(f"Failed to modify {self.model_class.__name__} items: {e}")
            raise

    def replace(self, filters: Dict[str, Any], new_items: List[T], session: Optional[Session] = None) -> ModelList:
        """
        Template method: Atomically replace items with new ones.

        Universal implementation for both ClickHouse and MySQL:
        1. Type validation for new items
        2. Query existing items that match filters
        3. If no existing items found, return empty result (no insertion)
        4. Delete existing items
        5. Insert new items
        6. Restore backup if insertion fails

        Args:
            filters: Dictionary of field -> value filters for items to replace
            new_items: List of new items to replace the filtered ones
            session: Optional SQLAlchemy session to use for the operation.

        Returns:
            ModelList of inserted items with their database identifiers
            Returns empty ModelList if no existing items match filters

        Raises:
            Exception: If the operation fails and restoration fails
        """
        if not filters:
            raise ValueError("Filters cannot be empty for replace operation")

        if not new_items:
            GLOG.WARN(f"No new items provided for {self.model_class.__name__} replace operation")
            return ModelList([], self)

        # Type validation: ensure all new_items are correct model type
        for item in new_items:
            if not isinstance(item, self.model_class):
                raise TypeError(
                    f"Item type mismatch: expected {self.model_class.__name__}, "
                    f"got {type(item).__name__}"
                )

        # Step 1: Query existing items
        backup_items = []
        try:
            existing_items = self.find(filters=filters, session=session)
            backup_items = list(existing_items)

            if len(backup_items) == 0:
                GLOG.INFO(
                    f"No existing {self.model_class.__name__} items found matching filters. "
                    f"No replacement performed."
                )
                # Return empty result without performing any insertion
                return ModelList([], self)

            GLOG.DEBUG(f"Found {len(backup_items)} existing {self.model_class.__name__} items to replace")

        except Exception as e:
            GLOG.ERROR(f"Failed to query existing {self.model_class.__name__} items: {e}")
            raise

        # Step 2: Delete existing items
        try:
            removed_count = self.remove(filters=filters, session=session)
            GLOG.DEBUG(f"Deleted {removed_count} existing {self.model_class.__name__} items")
        except Exception as e:
            GLOG.ERROR(f"Failed to delete {self.model_class.__name__} items: {e}")
            raise

        # Step 3: Insert new items
        try:
            inserted_items = self.add_batch(new_items, session=session)
            GLOG.INFO(
                f"Successfully replaced {len(backup_items)} with {len(new_items)} "
                f"{self.model_class.__name__} items"
            )
            return inserted_items
        except Exception as e:
            GLOG.ERROR(f"Failed to insert new {self.model_class.__name__} items, attempting restoration: {e}")

            # Step 4: Restore from backup if insertion fails
            try:
                if backup_items:
                    self.add_batch(backup_items, session=session)
                    GLOG.INFO(f"Successfully restored {len(backup_items)} backed up {self.model_class.__name__} items")
            except Exception as restore_error:
                GLOG.CRITICAL(
                    f"CRITICAL: Failed to restore {self.model_class.__name__} backup! "
                    f"Original error: {e}, Restore error: {restore_error}"
                )
                raise Exception(f"Replace operation failed and restoration failed: {restore_error}")

            raise Exception(f"Replace operation failed: {e}")

    def count(self, filters: Optional[Dict[str, Any]] = None, session: Optional[Session] = None) -> int:
        """
        Template method: Count items with optional filters.
        Subclasses should override _do_count() instead.

        Args:
            filters: Optional dictionary of field -> value filters
            session: Optional SQLAlchemy session to use for the operation.

        Returns:
            Number of matching records
        """
        try:
            return self._do_count(filters, session)
        except Exception as e:
            GLOG.ERROR(f"Failed to count {self.model_class.__name__} items: {e}")
            return 0

    def exists(self, filters: Optional[Dict[str, Any]] = None, session: Optional[Session] = None) -> bool:
        """
        Template method: Check if items exist with optional filters.
        Subclasses should override _do_exists() instead.

        Args:
            filters: Optional dictionary of field -> value filters
            session: Optional SQLAlchemy session to use for the operation.

        Returns:
            True if at least one matching record exists, False otherwise
        """
        try:
            return self._do_exists(filters, session)
        except Exception as e:
            GLOG.ERROR(f"Failed to check if {self.model_class.__name__} items exist: {e}")
            return False

    def soft_remove(self, filters: Dict[str, Any], session: Optional[Session] = None) -> None:
        """
        Template method: Soft remove items by filters.
        For MySQL: Sets is_del=True
        For ClickHouse: Calls remove() directly
        Subclasses should override _do_soft_remove() instead.

        Args:
            filters: Dictionary of field -> value filters for soft removal
            session: Optional SQLAlchemy session to use for the operation.
        """
        if not filters:
            GLOG.ERROR("Soft remove operation requires filters for safety")
            return

        try:
            return self._do_soft_remove(filters, session)
        except Exception as e:
            GLOG.ERROR(f"Failed to soft remove {self.model_class.__name__} items: {e}")
            raise

    # ============================================================================
    # Hook Methods - Subclasses can override these without worrying about decorators
    # ============================================================================

    def _validate_item_enum_fields(self, item: Any) -> Any:
        """
        ðŸŽ¯ Validate and convert enum fields in an item based on _get_enum_mappings().
        Ensures enum fields are properly converted to their integer values for database storage.

        Args:
            item: Item to validate (model instance, entity, or dict)

        Returns:
            Validated item with enum fields converted to integers
        """
        enum_mappings = self._get_enum_mappings()
        if not enum_mappings:
            return item  # No enum mappings, return as-is

        # Handle different item types
        if hasattr(item, '__dict__'):
            # Model instance or object with attributes
            for field, enum_class in enum_mappings.items():
                if hasattr(item, field):
                    value = getattr(item, field)
                    if value is not None:
                        converted_value = self._normalize_single_enum_value(value, enum_class, field)
                        if converted_value is not None:
                            try:
                                setattr(item, field, converted_value)
                            except AttributeError:
                                # Skip read-only properties (common in business entities)
                                from ginkgo.libs import GLOG
                                GLOG.DEBUG(f"Skipping read-only property {field} for {type(item).__name__}")
        elif isinstance(item, dict):
            # Dictionary
            for field, enum_class in enum_mappings.items():
                if field in item and item[field] is not None:
                    converted_value = self._normalize_single_enum_value(item[field], enum_class, field)
                    if converted_value is not None:
                        item[field] = converted_value

        return item

    def _do_add(self, item: T, session: Optional[Session] = None) -> T:
        """
        Hook method: Override to customize single item addition logic.
        """
        # ðŸŽ¯ Validate enum fields before adding to database
        validated_item = self._validate_item_enum_fields(item)

        if session:
            result = add(validated_item, session=session)
        else:
            conn = self._get_connection()
            with conn.get_session() as s:
                result = add(validated_item, session=s)
        GLOG.DEBUG(f"Added {self.model_class.__name__} item successfully")
        return result

    def _do_add_batch(self, items: List[Any], session: Optional[Session] = None) -> tuple[int, int]:
        """
        Hook method: Override to customize batch addition logic.
        """
        converted_items = self._convert_input_batch(items)

        if session:
            result = add_all(converted_items, session=session)
        else:
            conn = self._get_connection()
            with conn.get_session() as s:
                result = add_all(converted_items, session=s)

        GLOG.DEBUG(f"Added {len(converted_items)} {self.model_class.__name__} items in batch")
        return result

    def _do_find(
        self,
        filters: Optional[Dict[str, Any]] = None,
        page: Optional[int] = None,
        page_size: Optional[int] = None,
        order_by: Optional[str] = None,
        desc_order: bool = False,
        as_dataframe: bool = False,
        output_type: str = "model",
        distinct_field: Optional[str] = None,
        session: Optional[Session] = None,
    ) -> Union[List[Any], pd.DataFrame]:
        """
        Hook method: Override to customize find logic.
        """
        if session is None:
            conn = self._get_connection()
            session_context = conn.get_session()
        else:
            session_context = session

        with session_context as s:
            # Handle DISTINCT query for specific field
            if distinct_field and hasattr(self.model_class, distinct_field):
                from sqlalchemy import distinct

                field_attr = getattr(self.model_class, distinct_field)
                query = s.query(distinct(field_attr))

                # Apply enhanced filters (supports operators)
                if filters:
                    filter_conditions = self._parse_filters(filters)
                    if filter_conditions:
                        query = query.filter(and_(*filter_conditions))

                # Apply ordering for DISTINCT field
                if order_by and order_by == distinct_field:
                    if desc_order:
                        query = query.order_by(field_attr.desc())
                    else:
                        query = query.order_by(field_attr)

                # Apply pagination
                if page_size is not None:
                    if page is not None:
                        query = query.offset(page * page_size).limit(page_size)
                    else:
                        query = query.limit(page_size)  # é»˜è®¤ä»Žç¬¬0é¡µå¼€å§‹

                # Execute DISTINCT query
                results = query.all()
                distinct_values = [row[0] for row in results]

                # Clean ClickHouse FixedString null bytes for string fields
                if self._is_clickhouse and distinct_field in self.CLICKHOUSE_STRING_FIELDS:
                    distinct_values = self._clean_clickhouse_strings(distinct_values)

                GLOG.DEBUG(f"Found {len(distinct_values)} distinct {distinct_field} values")
                return distinct_values

            # Regular query (not DISTINCT)
            query = s.query(self.model_class)

            # Apply enhanced filters (supports operators)
            if filters:
                filter_conditions = self._parse_filters(filters)
                if filter_conditions:
                    query = query.filter(and_(*filter_conditions))

            # Apply ordering
            if order_by and hasattr(self.model_class, order_by):
                order_field = getattr(self.model_class, order_by)
                if desc_order:
                    query = query.order_by(order_field.desc())
                else:
                    query = query.order_by(order_field)

            # Apply pagination
            if page_size is not None:
                if page is not None:
                    query = query.offset(page * page_size).limit(page_size)
                else:
                    query = query.limit(page_size)  # é»˜è®¤ä»Žç¬¬0é¡µå¼€å§‹

            if as_dataframe:
                df = pd.read_sql(query.statement, s.connection())

                # Clean ClickHouse FixedString null bytes for specific fields
                if self._is_clickhouse and df.shape[0] > 0:
                    df = self._clean_clickhouse_strings(df)

                GLOG.DEBUG(f"Found {df.shape[0]} {self.model_class.__name__} records as DataFrame")
                return df
            else:
                results = query.all()
                GLOG.DEBUG(f"Found {len(results)} {self.model_class.__name__} records")

                # Detach objects from session with performance consideration
                if len(results) > 100:  # Large dataset: use batch expunge for performance
                    s.expunge_all()
                else:  # Small dataset: use precise expunge for safety
                    for obj in results:
                        s.expunge(obj)

                # Apply output conversion
                return self._convert_output_items(results, output_type)

    def _do_remove(self, filters: Dict[str, Any], session: Optional[Session] = None) -> None:
        """
        Hook method: Override to customize remove logic.
        """
        if session is None:
            conn = self._get_connection()
            session_context = conn.get_session()
        else:
            session_context = session

        with session_context as s:
            if self._is_clickhouse:
                # ClickHouse requires native SQL for DELETE operations
                from ginkgo.enums import EnumBase  # Import for enum conversion
                sql_parts = []
                params = {}

                for key, value in filters.items():
                    # Convert enum objects to integers for ClickHouse compatibility
                    if isinstance(value, EnumBase):
                        value = value.value
                    elif isinstance(value, list):
                        value = [item.value if isinstance(item, EnumBase) else item for item in value]
                    
                    if "__" in key:
                        field, operator = key.split("__", 1)
                        if hasattr(self.model_class, field):
                            param_name = f"param_{field}_{operator}"
                            if operator == "gte":
                                sql_parts.append(f"{field} >= :{param_name}")
                            elif operator == "lte":
                                sql_parts.append(f"{field} <= :{param_name}")
                            elif operator == "gt":
                                sql_parts.append(f"{field} > :{param_name}")
                            elif operator == "lt":
                                sql_parts.append(f"{field} < :{param_name}")
                            elif operator == "in":
                                # Handle IN operator specially
                                placeholders = [f":param_{field}_in_{i}" for i in range(len(value))]
                                sql_parts.append(f"{field} IN ({','.join(placeholders)})")
                                for i, val in enumerate(value):
                                    params[f"param_{field}_in_{i}"] = val
                                continue
                            elif operator == "like":
                                sql_parts.append(f"{field} LIKE :{param_name}")
                            params[param_name] = value
                    else:
                        # Standard equality filter
                        if hasattr(self.model_class, key):
                            param_name = f"param_{key}"
                            sql_parts.append(f"{key} = :{param_name}")
                            params[param_name] = value

                if sql_parts:
                    s.execute(
                        text(f"DELETE FROM {self.model_class.__tablename__} WHERE {' AND '.join(sql_parts)}"), params
                    )
                    GLOG.DEBUG(f"Deleted {self.model_class.__name__} records from ClickHouse")
            else:
                # MySQL can use SQLAlchemy ORM with enhanced filters
                filter_conditions = self._parse_filters(filters)
                if filter_conditions:
                    stmt = delete(self.model_class).where(and_(*filter_conditions))
                    s.execute(stmt)
                    GLOG.DEBUG(f"Deleted {self.model_class.__name__} records from MySQL")
            s.commit()

    def _do_modify(self, filters: Dict[str, Any], updates: Dict[str, Any], session: Optional[Session] = None) -> None:
        """
        Hook method: Override to customize modify logic.
        """
        if session is None:
            conn = self._get_connection()
            session_context = conn.get_session()
        else:
            session_context = session

        with session_context as s:
            # Build filter conditions
            filter_conditions = []
            for field, value in filters.items():
                if hasattr(self.model_class, field):
                    filter_conditions.append(getattr(self.model_class, field) == value)

            # Automatically update update_at timestamp for MySQL models
            if self._is_mysql and hasattr(self.model_class, "update_at"):
                import datetime

                updates = updates.copy()  # Don't modify original dict
                updates["update_at"] = datetime.datetime.now()

            # Execute update
            if filter_conditions:
                stmt = update(self.model_class).where(and_(*filter_conditions)).values(updates)
                s.execute(stmt)
                GLOG.DEBUG(f"Updated {self.model_class.__name__} records")

    def _do_count(self, filters: Optional[Dict[str, Any]] = None, session: Optional[Session] = None) -> int:
        """
        Hook method: Override to customize count logic.
        """
        if session is None:
            conn = self._get_connection()
            session_context = conn.get_session()
        else:
            session_context = session

        with session_context as s:
            query = s.query(self.model_class)

            if filters:
                filter_conditions = self._parse_filters(filters)
                if filter_conditions:
                    query = query.filter(and_(*filter_conditions))

            count = query.count()
            GLOG.DEBUG(f"Counted {count} {self.model_class.__name__} records")
            return count

    def _do_exists(self, filters: Optional[Dict[str, Any]] = None, session: Optional[Session] = None) -> bool:
        """
        Hook method: Override to customize exists logic.
        """
        if session is None:
            conn = self._get_connection()
            session_context = conn.get_session()
        else:
            session_context = session

        with session_context as s:
            query = s.query(self.model_class)

            if filters:
                filter_conditions = self._parse_filters(filters)
                if filter_conditions:
                    query = query.filter(and_(*filter_conditions))

            # Use exists() for better performance than count()
            exists = s.query(query.exists()).scalar()
            GLOG.DEBUG(f"Checked existence of {self.model_class.__name__} records: {exists}")
            # Explicitly convert to bool (ClickHouse may return 1/0 instead of True/False)
            return bool(exists)

    def _do_soft_remove(self, filters: Dict[str, Any], session: Optional[Session] = None) -> None:
        """
        Hook method: Override to customize soft remove logic.
        """
        if self._is_clickhouse:
            # ClickHouse: soft remove = hard remove
            GLOG.DEBUG(f"ClickHouse soft remove: calling remove() for {self.model_class.__name__}")
            self._do_remove(filters, session)  # Pass session to _do_remove
        else:
            # MySQL: set is_del=True and update_at
            import datetime

            updates = {"is_del": True, "update_at": datetime.datetime.now()}
            GLOG.DEBUG(f"MySQL soft remove: setting is_del=True and updating timestamp for {self.model_class.__name__}")
            self._do_modify(filters, updates, session)  # Pass session to _do_modify

    # ============================================================================
    # New Conversion and Utility Methods
    # ============================================================================

    def _create_from_params(self, **kwargs) -> T:
        """
        Hook method: Override to define how to create model from parameters.
        Called by create() template method.

        Args:
            **kwargs: Parameters to create the object

        Returns:
            Model instance

        Raises:
            NotImplementedError: Must be implemented by subclasses
        """
        raise NotImplementedError("Subclasses must implement _create_from_params")

    def _convert_input_batch(self, items: List[Any]) -> List[T]:
        """
        Convert a batch of input items to model instances.
        Attempts automatic conversion for each item.
        ðŸŽ¯ Also validates enum fields for all items.

        Args:
            items: List of input items (may be mixed types)

        Returns:
            List of converted model instances with validated enum fields
        """
        converted = []
        for item in items:
            if isinstance(item, self.model_class):
                # ðŸŽ¯ Validate enum fields for existing model instances
                validated_item = self._validate_item_enum_fields(item)
                converted.append(validated_item)
            else:
                # Try to convert using subclass conversion method
                converted_item = self._convert_input_item(item)
                if converted_item is not None:
                    # ðŸŽ¯ Validate enum fields for converted items
                    validated_item = self._validate_item_enum_fields(converted_item)
                    converted.append(validated_item)
                else:
                    GLOG.DEBUG(f"Cannot convert item {type(item)} to {self.model_class.__name__}")
        return converted

    def _convert_input_item(self, item: Any) -> Optional[T]:
        """
        Hook method: Override to support input type conversion.

        Args:
            item: Input item to convert

        Returns:
            Converted model instance or None if conversion not supported
        """
        return None  # Default: no conversion supported

    def _convert_output_items(self, items: List[T], output_type: str = "model") -> List[Any]:
        """
        Hook method: Override to support output type conversion.

        Args:
            items: List of model instances
            output_type: Desired output type

        Returns:
            List of converted output objects
        """
        return items  # Default: return model instances as-is

    def _get_enum_mappings(self) -> Dict[str, Any]:
        """
        ðŸŽ¯ Hook method: Override to define field-to-enum mappings.
        Subclasses should return a dictionary mapping field names to enum classes.

        Returns:
            Dictionary mapping field names to enum classes
            Example: {'market': MARKET_TYPES, 'currency': CURRENCY_TYPES}
        """
        return {}  # Default: no enum conversions

    def _process_dataframe_output(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ðŸŽ¯ Hook method: Process DataFrame output with enum conversions.
        Applies enum mappings to DataFrame columns.

        Args:
            df: Raw DataFrame from database

        Returns:
            DataFrame with enum fields properly converted
        """
        enum_mappings = self._get_enum_mappings()
        if not enum_mappings:
            return df

        df_converted = df.copy()
        for column, enum_class in enum_mappings.items():
            if column in df_converted.columns:
                df_converted[column] = df_converted[column].apply(
                    lambda x: self._safe_enum_convert(x, enum_class)
                )

        return df_converted

    def _convert_to_business_objects(self, raw_results: List[T]) -> List[Any]:
        """
        ðŸŽ¯ Hook method: Convert raw models to business objects.
        First fixes enum fields, then calls business object conversion hook.

        Args:
            raw_results: List of raw model instances from database

        Returns:
            List of converted business objects
        """
        # First fix enum fields in raw models
        enum_mappings = self._get_enum_mappings()
        for model in raw_results:
            for column, enum_class in enum_mappings.items():
                if hasattr(model, column):
                    current_value = getattr(model, column)
                    converted_value = self._safe_enum_convert(current_value, enum_class)
                    if converted_value is not None:
                        setattr(model, column, converted_value)

        # Then call business object conversion hook
        return self._convert_models_to_business_objects(raw_results)

    def _convert_models_to_business_objects(self, models: List[T]) -> List[Any]:
        """
        ðŸŽ¯ Hook method: Convert enum-fixed models to business objects.
        Subclasses should override this method to implement specific business logic.

        Args:
            models: List of models with enum fields already fixed

        Returns:
            List of business objects
        """
        return models  # Default: return models as-is

    def _convert_models_to_dataframe(self, models: List[T]) -> pd.DataFrame:
        """
        ðŸŽ¯ Convert models to pandas DataFrame with enum conversion.

        Args:
            models: List of model instances

        Returns:
            pandas DataFrame with enum fields converted to their proper representation
        """
        if not models:
            return pd.DataFrame()

        # First fix enum fields in models
        enum_mappings = self._get_enum_mappings()
        for model in models:
            for column, enum_class in enum_mappings.items():
                if hasattr(model, column):
                    current_value = getattr(model, column)
                    converted_value = self._safe_enum_convert(current_value, enum_class)
                    if converted_value is not None:
                        setattr(model, column, converted_value)

        # Convert to DataFrame
        data = []
        for model in models:
            model_dict = model.__dict__.copy()
            # Remove SQLAlchemy internal state
            model_dict.pop('_sa_instance_state', None)
            data.append(model_dict)

        return pd.DataFrame(data)

    def _safe_enum_convert(self, value, enum_class):
        """
        Utility method: Safe enum conversion with error handling.

        Args:
            value: Value to convert (typically int)
            enum_class: Enum class to convert to

        Returns:
            Enum instance or original value if conversion fails
        """
        try:
            if value is None:
                return None
            return enum_class(value)
        except (ValueError, TypeError):
            return value  # Return original value if conversion fails

    def _convert_enum_values(self, filters: Dict[str, Any]) -> Dict[str, Any]:
        """
        ðŸŽ¯ Convert enum values based on _get_enum_mappings() for precise enum handling.
        Only processes fields defined in enum_mappings, avoiding unnecessary type checks.

        Args:
            filters: Original filters dictionary

        Returns:
            Filters dictionary with enum values converted to integers
        """
        enum_mappings = self._get_enum_mappings()
        if not enum_mappings:
            return filters  # No enum mappings, return as-is

        converted_filters = filters.copy()

        for field, enum_class in enum_mappings.items():
            # Handle direct field matches
            if field in converted_filters:
                value = converted_filters[field]
                converted_filters[field] = self._normalize_single_enum_value(value, enum_class, field)

            # Handle operator suffixed fields (e.g., status__in, direction__gte)
            for suffix in ['__gte', '__lte', '__gt', '__lt', '__in', '__like']:
                field_with_suffix = field + suffix
                if field_with_suffix in converted_filters:
                    value = converted_filters[field_with_suffix]
                    converted_filters[field_with_suffix] = self._normalize_single_enum_value(value, enum_class, field)

        return converted_filters

    def _normalize_single_enum_value(self, value, enum_class, field_name: str):
        """
        ðŸŽ¯ Normalize a single enum value based on the expected enum class.

        Args:
            value: The value to normalize (enum, int, or list)
            enum_class: The expected enum class
            field_name: Field name for logging purposes

        Returns:
            Normalized value (enum converted to int, int validated, or original value)
        """
        if value is None:
            return None

        if isinstance(value, enum_class):
            # Convert enum to its integer value
            return value.value
        elif isinstance(value, list):
            # Handle lists containing enum values
            return [
                item.value if isinstance(item, enum_class) else item
                for item in value if item is not None
            ]
        elif isinstance(value, int):
            # Validate that the integer is a valid enum value
            try:
                enum_class(value)  # This will raise ValueError if invalid
                return value
            except ValueError:
                from ginkgo.libs import GLOG
                GLOG.WARN(f"Invalid enum value {value} for field {field_name}, expected {enum_class.__name__}")
                return value  # Return original value instead of None
        else:
            # Not an enum field value, return as-is
            return value

    def _parse_filters(self, filters: Dict[str, Any]) -> List[Any]:
        """
        Parse enhanced filters with operator support.
        Supports operators: gte, lte, gt, lt, in, like
        Uses _get_enum_mappings() for precise enum-to-integer conversion.

        Args:
            filters: Dictionary with field__operator keys
                   Examples: {"timestamp__gte": "2023-01-01", "volume__in": [100, 200]}

        Returns:
            List of SQLAlchemy filter conditions
        """
        # ðŸŽ¯ First, convert enum values based on mappings for precise handling
        converted_filters = self._convert_enum_values(filters)

        conditions = []

        for key, value in converted_filters.items():
            if "__" in key:
                field, operator = key.split("__", 1)
                if hasattr(self.model_class, field):
                    attr = getattr(self.model_class, field)
                    if operator == "gte":
                        conditions.append(attr >= value)
                    elif operator == "lte":
                        conditions.append(attr <= value)
                    elif operator == "gt":
                        conditions.append(attr > value)
                    elif operator == "lt":
                        conditions.append(attr < value)
                    elif operator == "in":
                        conditions.append(attr.in_(value))
                    elif operator == "like":
                        conditions.append(attr.like(f"%{value}%"))
                    else:
                        from ginkgo.libs import GLOG
                        GLOG.DEBUG(f"Unknown filter operator: {operator}")
            else:
                # Standard equality filter
                if hasattr(self.model_class, key):
                    conditions.append(getattr(self.model_class, key) == value)

        return conditions

    # ============================================================================
    # æµå¼æŸ¥è¯¢åŠŸèƒ½ - æ–°å¢žåŠŸèƒ½ï¼Œå®Œå…¨å‘åŽå…¼å®¹
    # ============================================================================

    def __init__(self, model_class: type[T]):
        # è°ƒç”¨åŽŸæœ‰åˆå§‹åŒ–é€»è¾‘
        if hasattr(super(), '__init__'):
            super().__init__()
        
        self.model_class = model_class
        self._is_clickhouse = issubclass(model_class, MClickBase)
        self._is_mysql = issubclass(model_class, MMysqlBase)

        if not (self._is_clickhouse or self._is_mysql):
            raise ValueError(f"Model {model_class} must inherit from MClickBase or MMysqlBase")

        # ðŸ†• æµå¼æŸ¥è¯¢ç›¸å…³å±žæ€§ï¼ˆé»˜è®¤ç¦ç”¨ï¼Œä¸å½±å“çŽ°æœ‰åŠŸèƒ½ï¼‰
        self._streaming_enabled = False
        self._streaming_engine = None
        self._streaming_config = None
        
        # å»¶è¿ŸåŠ è½½æµå¼æŸ¥è¯¢é…ç½®ï¼ˆåªæœ‰åœ¨ä½¿ç”¨æ—¶æ‰åŠ è½½ï¼‰
        self._streaming_initialized = False

    def _initialize_streaming(self) -> None:
        """å»¶è¿Ÿåˆå§‹åŒ–æµå¼æŸ¥è¯¢åŠŸèƒ½"""
        if self._streaming_initialized:
            return
            
        try:
            # å¯¼å…¥æµå¼æŸ¥è¯¢æ¨¡å—ï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªçŽ¯ä¾èµ–ï¼‰
            from ginkgo.data.streaming.config import get_config
            from ginkgo.data.streaming.engines import BaseStreamingEngine
            
            # åŠ è½½é…ç½®
            self._streaming_config = get_config()
            self._streaming_enabled = self._streaming_config.enabled
            
            if self._streaming_enabled:
                GLOG.DEBUG(f"Streaming functionality enabled for {self.model_class.__name__}")
            else:
                GLOG.DEBUG(f"Streaming functionality disabled for {self.model_class.__name__}")
                
            self._streaming_initialized = True
            
        except Exception as e:
            GLOG.WARNING(f"Failed to initialize streaming for {self.model_class.__name__}: {e}")
            self._streaming_enabled = False
            self._streaming_initialized = True

    def _get_streaming_engine(self):
        """èŽ·å–æµå¼æŸ¥è¯¢å¼•æ“Ž"""
        if not self._streaming_initialized:
            self._initialize_streaming()
            
        if not self._streaming_enabled:
            raise RuntimeError(
                "Streaming functionality is disabled. "
                "Enable it in config: streaming.enabled = true"
            )
        
        if self._streaming_engine is None:
            try:
                # å»¶è¿Ÿå¯¼å…¥å’Œåˆ›å»ºå¼•æ“Ž
                if self._is_mysql:
                    from ginkgo.data.streaming.engines.mysql_streaming_engine import MySQLStreamingEngine
                    self._streaming_engine = MySQLStreamingEngine(
                        self._get_connection(), 
                        self._streaming_config
                    )
                elif self._is_clickhouse:
                    from ginkgo.data.streaming.engines.clickhouse_streaming_engine import ClickHouseStreamingEngine
                    self._streaming_engine = ClickHouseStreamingEngine(
                        self._get_connection(),
                        self._streaming_config
                    )
                else:
                    raise RuntimeError(f"No streaming engine available for {self.model_class}")
                    
                GLOG.DEBUG(f"Created streaming engine for {self.model_class.__name__}")
                
            except ImportError as e:
                # å¦‚æžœå¼•æ“Žå°šæœªå®žçŽ°ï¼Œæä¾›å‹å¥½çš„é”™è¯¯ä¿¡æ¯
                raise RuntimeError(
                    f"Streaming engine not available for {self.model_class}. "
                    f"This feature is still under development: {e}"
                )
        
        return self._streaming_engine

    @time_logger
    def stream_find(self, 
                   filters: Optional[Dict[str, Any]] = None,
                   batch_size: Optional[int] = None,
                   order_by: Optional[str] = None,
                   desc_order: bool = False) -> Any:
        """
        ðŸ†• æµå¼æŸ¥è¯¢æŽ¥å£ - æ–°å¢žåŠŸèƒ½ï¼Œä¸å½±å“çŽ°æœ‰find()æ–¹æ³•
        
        æä¾›é«˜æ€§èƒ½çš„æµå¼æŸ¥è¯¢ï¼Œé€‚ç”¨äºŽå¤§æ•°æ®é›†å¤„ç†ã€‚
        å†…å­˜å ç”¨ç¨³å®šï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œè¿›åº¦ç›‘æŽ§ã€‚
        
        Args:
            filters: æŸ¥è¯¢è¿‡æ»¤æ¡ä»¶ï¼ˆæ”¯æŒæ“ä½œç¬¦ï¼Œå¦‚field__gteï¼‰
            batch_size: æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
            order_by: æŽ’åºå­—æ®µ
            desc_order: æ˜¯å¦é™åº
            
        Yields:
            Iterator[List[T]]: æ‰¹æ¬¡æ•°æ®è¿­ä»£å™¨
            
        Example:
            >>> # æµå¼æŸ¥è¯¢500ä¸‡æ¡Kçº¿æ•°æ®
            >>> for batch in bar_crud.stream_find(
            ...     filters={'timestamp__gte': '2020-01-01'},
            ...     batch_size=1000
            ... ):
            ...     process_batch(batch)  # å¤„ç†1000æ¡æ•°æ®
        """
        try:
            # èŽ·å–æµå¼æŸ¥è¯¢å¼•æ“Ž
            engine = self._get_streaming_engine()
            
            # æž„å»ºåŸºç¡€æŸ¥è¯¢
            base_query = self._build_streaming_query(filters, order_by, desc_order)
            
            # æ‰§è¡Œæµå¼æŸ¥è¯¢
            return engine.execute_stream(
                query=base_query,
                filters=filters or {},
                batch_size=batch_size
            )
            
        except Exception as e:
            GLOG.ERROR(f"Stream find failed for {self.model_class.__name__}: {e}")
            
            # è‡ªåŠ¨é™çº§åˆ°ä¼ ç»ŸæŸ¥è¯¢ï¼ˆå¦‚æžœå¯ç”¨äº†é™çº§ï¼‰
            if self._streaming_config and self._streaming_config.recovery.enable_fallback:
                GLOG.WARNING(f"Falling back to traditional query for {self.model_class.__name__}")
                return self._fallback_to_traditional_query(filters, batch_size, order_by, desc_order)
            else:
                raise

    def stream_find_with_progress(self,
                                 filters: Optional[Dict[str, Any]] = None,
                                 batch_size: Optional[int] = None,
                                 progress_callback: Optional[Any] = None,
                                 **kwargs) -> Any:
        """
        ðŸ†• å¸¦è¿›åº¦å›žè°ƒçš„æµå¼æŸ¥è¯¢
        
        Args:
            filters: æŸ¥è¯¢è¿‡æ»¤æ¡ä»¶
            batch_size: æ‰¹æ¬¡å¤§å°
            progress_callback: è¿›åº¦å›žè°ƒå‡½æ•° callback(progress_info)
            **kwargs: å…¶ä»–å‚æ•°
            
        Yields:
            Iterator[List[T]]: æ‰¹æ¬¡æ•°æ®è¿­ä»£å™¨
        """
        # èŽ·å–å¼•æ“Žå¹¶æ·»åŠ è¿›åº¦è§‚å¯Ÿè€…
        engine = self._get_streaming_engine()
        
        if progress_callback:
            # åˆ›å»ºè¿›åº¦è§‚å¯Ÿè€…
            from ginkgo.data.streaming.engines.base_streaming_engine import ProgressObserver
            
            class CallbackObserver(ProgressObserver):
                def __init__(self, callback):
                    self.callback = callback
                    
                def on_progress_update(self, progress):
                    self.callback(progress)
                    
                def on_batch_processed(self, batch_index, batch_size):
                    pass
                    
                def on_error(self, error):
                    pass
            
            observer = CallbackObserver(progress_callback)
            engine.add_observer(observer)
        
        try:
            yield from self.stream_find(filters, batch_size, **kwargs)
        finally:
            # æ¸…ç†è§‚å¯Ÿè€…
            if progress_callback:
                engine.remove_observer(observer)

    def stream_find_resumable(self,
                             query_id: str,
                             filters: Optional[Dict[str, Any]] = None,
                             batch_size: Optional[int] = None,
                             **kwargs) -> Any:
        """
        ðŸ†• æ”¯æŒæ–­ç‚¹ç»­ä¼ çš„æµå¼æŸ¥è¯¢
        
        Args:
            query_id: æŸ¥è¯¢å”¯ä¸€æ ‡è¯†ç¬¦
            filters: æŸ¥è¯¢è¿‡æ»¤æ¡ä»¶
            batch_size: æ‰¹æ¬¡å¤§å°
            **kwargs: å…¶ä»–å‚æ•°
            
        Yields:
            Iterator[List[T]]: æ‰¹æ¬¡æ•°æ®è¿­ä»£å™¨
        """
        engine = self._get_streaming_engine()
        
        # å°è¯•åŠ è½½æ–­ç‚¹çŠ¶æ€
        checkpoint_state = None
        if self._streaming_config.recovery.enable_checkpoint:
            try:
                from ginkgo.data.streaming.managers.checkpoint_manager import CheckpointManager
                # è¿™é‡Œéœ€è¦Redisè¿žæŽ¥ï¼Œæš‚æ—¶è·³è¿‡å…·ä½“å®žçŽ°
                # checkpoint_manager = CheckpointManager(redis_client)
                # checkpoint_state = checkpoint_manager.load_checkpoint(query_id)
                pass
            except Exception as e:
                GLOG.WARNING(f"Failed to load checkpoint for query {query_id}: {e}")
        
        # æž„å»ºæŸ¥è¯¢
        base_query = self._build_streaming_query(filters, **kwargs)
        
        # æ‰§è¡Œå¸¦æ–­ç‚¹ç»­ä¼ çš„æµå¼æŸ¥è¯¢
        return engine.execute_stream_with_checkpoint(
            query=base_query,
            checkpoint_state=checkpoint_state,
            filters=filters,
            batch_size=batch_size
        )

    def is_streaming_enabled(self) -> bool:
        """æ£€æŸ¥æµå¼æŸ¥è¯¢æ˜¯å¦å·²å¯ç”¨"""
        if not self._streaming_initialized:
            self._initialize_streaming()
        return self._streaming_enabled

    def enable_streaming(self) -> None:
        """è¿è¡Œæ—¶å¯ç”¨æµå¼æŸ¥è¯¢"""
        if not self._streaming_initialized:
            self._initialize_streaming()
        self._streaming_enabled = True
        GLOG.INFO(f"Streaming enabled for {self.model_class.__name__}")

    def disable_streaming(self) -> None:
        """è¿è¡Œæ—¶ç¦ç”¨æµå¼æŸ¥è¯¢"""
        self._streaming_enabled = False
        self._streaming_engine = None
        GLOG.INFO(f"Streaming disabled for {self.model_class.__name__}")

    def get_streaming_metrics(self) -> Optional[Any]:
        """èŽ·å–æµå¼æŸ¥è¯¢æ€§èƒ½æŒ‡æ ‡"""
        if self._streaming_engine:
            return self._streaming_engine.get_metrics()
        return None

    # ==================== å†…éƒ¨è¾…åŠ©æ–¹æ³• ====================

    def _build_streaming_query(self, 
                              filters: Optional[Dict[str, Any]] = None,
                              order_by: Optional[str] = None,
                              desc_order: bool = False) -> str:
        """æž„å»ºæµå¼æŸ¥è¯¢SQLè¯­å¥"""
        # åŸºç¡€SELECTè¯­å¥
        table_name = self.model_class.__tablename__
        query = f"SELECT * FROM {table_name}"
        
        # æ·»åŠ WHEREæ¡ä»¶
        if filters:
            where_conditions = []
            for key, value in filters.items():
                if "__" in key:
                    field, operator = key.split("__", 1)
                    if hasattr(self.model_class, field):
                        if operator == "gte":
                            where_conditions.append(f"{field} >= '{value}'")
                        elif operator == "lte":
                            where_conditions.append(f"{field} <= '{value}'")
                        elif operator == "gt":
                            where_conditions.append(f"{field} > '{value}'")
                        elif operator == "lt":
                            where_conditions.append(f"{field} < '{value}'")
                        elif operator == "in":
                            if isinstance(value, (list, tuple)):
                                value_str = "', '".join(str(v) for v in value)
                                where_conditions.append(f"{field} IN ('{value_str}')")
                        elif operator == "like":
                            where_conditions.append(f"{field} LIKE '%{value}%'")
                else:
                    if hasattr(self.model_class, key):
                        where_conditions.append(f"{key} = '{value}'")
            
            if where_conditions:
                query += f" WHERE {' AND '.join(where_conditions)}"
        
        # æ·»åŠ ORDER BY
        if order_by and hasattr(self.model_class, order_by):
            query += f" ORDER BY {order_by}"
            if desc_order:
                query += " DESC"
        
        return query

    def _fallback_to_traditional_query(self,
                                     filters: Optional[Dict[str, Any]] = None,
                                     batch_size: Optional[int] = None,
                                     order_by: Optional[str] = None,
                                     desc_order: bool = False) -> Any:
        """é™çº§åˆ°ä¼ ç»ŸæŸ¥è¯¢çš„å®žçŽ°"""
        GLOG.INFO(f"Using traditional query fallback for {self.model_class.__name__}")
        
        # ä½¿ç”¨çŽ°æœ‰çš„findæ–¹æ³•ï¼Œåˆ†é¡µè¿”å›ž
        batch_size = batch_size or 1000
        page = 0
        
        while True:
            batch = self.find(
                filters=filters,
                page=page,
                page_size=batch_size,
                order_by=order_by,
                desc_order=desc_order
            )
            
            if not batch:
                break
                
            yield batch
            page += 1
            
            # é¿å…æ— é™å¾ªçŽ¯
            if len(batch) < batch_size:
                break

    # ==================== ðŸ†• æ–­ç‚¹ç»­ä¼ æµå¼æŸ¥è¯¢æ–¹æ³• ====================
    
    def stream_find_resumable(self,
                             filters: Optional[Dict[str, Any]] = None,
                             batch_size: Optional[int] = None,
                             order_by: Optional[str] = "timestamp",
                             desc_order: bool = False,
                             checkpoint_id: Optional[str] = None,
                             auto_checkpoint: bool = True,
                             checkpoint_interval: int = 1000) -> Any:
        """
        ðŸ†• æ”¯æŒæ–­ç‚¹ç»­ä¼ çš„æµå¼æŸ¥è¯¢
        
        Args:
            filters: æŸ¥è¯¢è¿‡æ»¤æ¡ä»¶
            batch_size: æ‰¹æ¬¡å¤§å°
            order_by: æŽ’åºå­—æ®µï¼ˆç”¨äºŽæ–­ç‚¹ç»­ä¼ ï¼‰
            desc_order: æ˜¯å¦é™åº
            checkpoint_id: çŽ°æœ‰æ–­ç‚¹IDï¼ˆç”¨äºŽæ¢å¤ï¼‰
            auto_checkpoint: æ˜¯å¦è‡ªåŠ¨åˆ›å»ºæ–­ç‚¹
            checkpoint_interval: è‡ªåŠ¨æ–­ç‚¹é—´éš”
            
        Yields:
            æŸ¥è¯¢ç»“æžœæ‰¹æ¬¡
        """
        if not self._streaming_enabled:
            GLOG.WARNING("Streaming not enabled, falling back to traditional query")
            yield from self._fallback_to_traditional_query(filters, batch_size, order_by, desc_order)
            return
        
        try:
            # å¯¼å…¥æ–­ç‚¹ç®¡ç†å™¨ï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªçŽ¯ä¾èµ–ï¼‰
            from ginkgo.data.streaming.checkpoint import checkpoint_manager, progress_tracking_manager
            from ginkgo.data.streaming import StreamingState, CheckpointError
            
            # æ¢å¤æˆ–åˆ›å»ºæ–­ç‚¹
            checkpoint = None
            if checkpoint_id:
                checkpoint = checkpoint_manager.get_checkpoint(checkpoint_id)
                if not checkpoint:
                    raise CheckpointError(f"Checkpoint not found: {checkpoint_id}")
                GLOG.INFO(f"Resuming from checkpoint: {checkpoint_id}")
            elif auto_checkpoint:
                # åˆ›å»ºæ–°æ–­ç‚¹
                checkpoint_id = checkpoint_manager.create_checkpoint(
                    query=self._build_streaming_query(filters, order_by, desc_order),
                    filters=filters or {},
                    batch_size=batch_size or 1000,
                    database_type=getattr(self.driver, '_db_type', 'unknown'),
                    engine_type='streaming',
                    estimated_total=self._estimate_total_records(filters)
                )
                checkpoint = checkpoint_manager.get_checkpoint(checkpoint_id)
                GLOG.INFO(f"Created checkpoint: {checkpoint_id}")
            
            # è°ƒæ•´è¿‡æ»¤æ¡ä»¶æ”¯æŒæ–­ç‚¹ç»­ä¼ 
            adjusted_filters = self._adjust_filters_for_resume(filters, checkpoint, order_by)
            
            # åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
            tracker_id = None
            if checkpoint:
                from ginkgo.data.streaming.checkpoint import checkpoint_manager
                query_hash = checkpoint_manager._generate_query_hash(
                    checkpoint.query_text, 
                    adjusted_filters
                )
                tracker_id = progress_tracking_manager.create_tracker(
                    query_hash=query_hash,
                    estimated_total=checkpoint.estimated_total
                )
            
            # æ‰§è¡Œæµå¼æŸ¥è¯¢
            processed_in_session = 0
            total_processed = checkpoint.processed_count if checkpoint else 0
            
            try:
                # æ›´æ–°æ–­ç‚¹çŠ¶æ€ä¸ºè¿è¡Œä¸­
                if checkpoint:
                    checkpoint_manager.update_checkpoint(
                        checkpoint.checkpoint_id,
                        state=StreamingState.RUNNING
                    )
                
                for batch in self.stream_find(
                    filters=adjusted_filters,
                    batch_size=batch_size,
                    order_by=order_by,
                    desc_order=desc_order
                ):
                    # æ›´æ–°å¤„ç†è®¡æ•°
                    batch_size_actual = len(batch)
                    processed_in_session += batch_size_actual
                    total_processed += batch_size_actual
                    
                    # æ›´æ–°è¿›åº¦è·Ÿè¸ª
                    if tracker_id:
                        progress_tracking_manager.update_progress(
                            tracker_id=tracker_id,
                            processed_count=total_processed,
                            batch_size=batch_size_actual
                        )
                    
                    # è‡ªåŠ¨ä¿å­˜æ–­ç‚¹
                    if checkpoint and auto_checkpoint and processed_in_session >= checkpoint_interval:
                        self._save_checkpoint_progress(
                            checkpoint, batch, total_processed, order_by
                        )
                        processed_in_session = 0
                    
                    yield batch
                
                # æŸ¥è¯¢å®Œæˆï¼Œæ›´æ–°æ–­ç‚¹çŠ¶æ€
                if checkpoint:
                    checkpoint_manager.update_checkpoint(
                        checkpoint.checkpoint_id,
                        state=StreamingState.COMPLETED,
                        processed_count=total_processed,
                        progress_percentage=100.0
                    )
                    GLOG.INFO(f"Streaming query completed. Checkpoint: {checkpoint.checkpoint_id}")
                
            except Exception as e:
                # æŸ¥è¯¢å¤±è´¥ï¼Œä¿å­˜å½“å‰è¿›åº¦
                if checkpoint:
                    checkpoint_manager.update_checkpoint(
                        checkpoint.checkpoint_id,
                        state=StreamingState.FAILED,
                        processed_count=total_processed
                    )
                    GLOG.ERROR(f"Streaming query failed. Checkpoint saved: {checkpoint.checkpoint_id}")
                raise
            finally:
                # æ¸…ç†è¿›åº¦è·Ÿè¸ªå™¨
                if tracker_id:
                    progress_tracking_manager.remove_tracker(tracker_id)
        
        except Exception as e:
            GLOG.ERROR(f"Resumable streaming query failed: {e}")
            # é™çº§åˆ°æ™®é€šæµå¼æŸ¥è¯¢
            GLOG.INFO("Falling back to regular streaming query")
            yield from self.stream_find(filters, batch_size, order_by, desc_order)
    
    def stream_find_with_detailed_progress(self,
                                         filters: Optional[Dict[str, Any]] = None,
                                         batch_size: Optional[int] = None,
                                         progress_callback: Optional[Callable] = None,
                                         checkpoint_id: Optional[str] = None) -> Any:
        """
        ðŸ†• å¸¦è¯¦ç»†è¿›åº¦ç›‘æŽ§çš„æµå¼æŸ¥è¯¢
        
        Args:
            filters: æŸ¥è¯¢è¿‡æ»¤æ¡ä»¶
            batch_size: æ‰¹æ¬¡å¤§å°
            progress_callback: è¿›åº¦å›žè°ƒå‡½æ•°
            checkpoint_id: æ–­ç‚¹IDï¼ˆç”¨äºŽæ¢å¤ï¼‰
            
        Yields:
            æŸ¥è¯¢ç»“æžœæ‰¹æ¬¡
        """
        # æ·»åŠ é»˜è®¤è¿›åº¦å›žè°ƒ
        def default_progress_callback(progress_info):
            from ginkgo.data.streaming import ProgressInfo
            GLOG.INFO(
                f"Streaming progress: {progress_info.processed} processed, "
                f"rate: {progress_info.rate:.1f} records/sec, "
                f"elapsed: {progress_info.elapsed:.1f}s"
                + (f", progress: {progress_info.progress_percentage:.1f}%" 
                   if progress_info.progress_percentage else "")
            )
        
        actual_callback = progress_callback or default_progress_callback
        
        # ä½¿ç”¨æ–­ç‚¹ç»­ä¼ åŠŸèƒ½
        for batch in self.stream_find_resumable(
            filters=filters,
            batch_size=batch_size,
            checkpoint_id=checkpoint_id,
            auto_checkpoint=True
        ):
            # æ‰§è¡Œè¿›åº¦å›žè°ƒ
            if actual_callback:
                try:
                    # è¿™é‡Œéœ€è¦æž„é€ ProgressInfoï¼Œå®žé™…å®žçŽ°ä¸­ä¼šä»Žè·Ÿè¸ªå™¨èŽ·å–
                    from ginkgo.data.streaming import ProgressInfo
                    progress_info = ProgressInfo(
                        processed=len(batch),  # ç®€åŒ–å®žçŽ°
                        rate=0.0,
                        elapsed=0.0
                    )
                    actual_callback(progress_info)
                except Exception as e:
                    GLOG.WARNING(f"Progress callback failed: {e}")
            
            yield batch
    
    def get_checkpoint_status(self, checkpoint_id: str) -> Optional[Dict[str, Any]]:
        """
        ðŸ†• èŽ·å–æ–­ç‚¹çŠ¶æ€ä¿¡æ¯
        
        Args:
            checkpoint_id: æ–­ç‚¹ID
            
        Returns:
            æ–­ç‚¹çŠ¶æ€ä¿¡æ¯
        """
        try:
            from ginkgo.data.streaming.checkpoint import checkpoint_manager
            
            checkpoint = checkpoint_manager.get_checkpoint(checkpoint_id)
            if not checkpoint:
                return None
            
            return {
                "checkpoint_id": checkpoint.checkpoint_id,
                "state": checkpoint.state.value,
                "processed_count": checkpoint.processed_count,
                "progress_percentage": checkpoint.progress_percentage,
                "processing_rate": checkpoint.processing_rate,
                "elapsed_time": checkpoint.elapsed_time,
                "created_at": checkpoint.created_at,
                "updated_at": checkpoint.updated_at,
                "database_type": checkpoint.database_type,
                "engine_type": checkpoint.engine_type
            }
            
        except Exception as e:
            GLOG.ERROR(f"Failed to get checkpoint status: {e}")
            return None
    
    def list_checkpoints(self) -> List[Dict[str, Any]]:
        """
        ðŸ†• åˆ—å‡ºæ‰€æœ‰ç›¸å…³æ–­ç‚¹
        
        Returns:
            æ–­ç‚¹åˆ—è¡¨
        """
        try:
            from ginkgo.data.streaming.checkpoint import checkpoint_manager
            
            checkpoints = checkpoint_manager.list_checkpoints()
            return [
                {
                    "checkpoint_id": cp.checkpoint_id,
                    "state": cp.state.value,
                    "processed_count": cp.processed_count,
                    "progress_percentage": cp.progress_percentage,
                    "created_at": cp.created_at,
                    "database_type": cp.database_type
                }
                for cp in checkpoints
            ]
            
        except Exception as e:
            GLOG.ERROR(f"Failed to list checkpoints: {e}")
            return []
    
    def delete_checkpoint(self, checkpoint_id: str) -> bool:
        """
        ðŸ†• åˆ é™¤æ–­ç‚¹
        
        Args:
            checkpoint_id: æ–­ç‚¹ID
            
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            from ginkgo.data.streaming.checkpoint import checkpoint_manager
            return checkpoint_manager.delete_checkpoint(checkpoint_id)
        except Exception as e:
            GLOG.ERROR(f"Failed to delete checkpoint: {e}")
            return False
    
    def _adjust_filters_for_resume(self, 
                                  filters: Optional[Dict[str, Any]], 
                                  checkpoint: Optional[Any],
                                  order_by: str) -> Dict[str, Any]:
        """è°ƒæ•´è¿‡æ»¤æ¡ä»¶ä»¥æ”¯æŒæ–­ç‚¹ç»­ä¼ """
        adjusted_filters = dict(filters) if filters else {}
        
        if checkpoint and checkpoint.last_timestamp:
            # ä½¿ç”¨æ—¶é—´æˆ³æ–­ç‚¹ç»­ä¼ 
            timestamp_filter = f"{order_by}__gt"
            adjusted_filters[timestamp_filter] = checkpoint.last_timestamp
            GLOG.DEBUG(f"Resuming from timestamp: {checkpoint.last_timestamp}")
        elif checkpoint and checkpoint.last_offset > 0:
            # ä½¿ç”¨åç§»é‡æ–­ç‚¹ç»­ä¼ ï¼ˆä¸å¤ªç²¾ç¡®ï¼Œä½†æ€»æ¯”æ²¡æœ‰å¥½ï¼‰
            GLOG.DEBUG(f"Resuming from offset: {checkpoint.last_offset}")
        
        return adjusted_filters
    
    def _save_checkpoint_progress(self, 
                                checkpoint: Any, 
                                batch: List[Any], 
                                total_processed: int,
                                order_by: str):
        """ä¿å­˜æ–­ç‚¹è¿›åº¦"""
        try:
            from ginkgo.data.streaming.checkpoint import checkpoint_manager
            
            updates = {
                "processed_count": total_processed,
                "last_offset": total_processed
            }
            
            # å°è¯•ä»Žæ‰¹æ¬¡æ•°æ®ä¸­æå–æ—¶é—´æˆ³
            if batch and hasattr(batch[0], order_by):
                last_timestamp = getattr(batch[-1], order_by)
                if last_timestamp:
                    updates["last_timestamp"] = str(last_timestamp)
            
            checkpoint_manager.update_checkpoint(
                checkpoint.checkpoint_id, 
                **updates
            )
            
            GLOG.DEBUG(f"Saved checkpoint progress: {total_processed} records processed")
            
        except Exception as e:
            GLOG.WARNING(f"Failed to save checkpoint progress: {e}")
    
    def _estimate_total_records(self, filters: Optional[Dict[str, Any]]) -> Optional[int]:
        """ä¼°ç®—æ€»è®°å½•æ•°ï¼ˆç”¨äºŽè¿›åº¦è®¡ç®—ï¼‰"""
        try:
            # ç®€å•çš„ä¼°ç®—æ–¹æ³•ï¼šæ‰§è¡ŒCOUNTæŸ¥è¯¢
            # å®žé™…ç”Ÿäº§çŽ¯å¢ƒå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ä¼°ç®—é€»è¾‘
            count_result = self.count(filters=filters)
            return count_result if isinstance(count_result, int) else None
        except Exception as e:
            GLOG.DEBUG(f"Failed to estimate total records: {e}")
            return None

    # ==================== ðŸ†• å†…å­˜ç›‘æŽ§å’Œä¼šè¯ç®¡ç†é›†æˆ ====================
    
    def stream_find_with_monitoring(self,
                                   filters: Optional[Dict[str, Any]] = None,
                                   batch_size: Optional[int] = None,
                                   order_by: Optional[str] = None,
                                   desc_order: bool = False,
                                   memory_limit_mb: Optional[float] = None,
                                   auto_optimize: bool = True) -> Any:
        """
        ðŸ†• å¸¦å†…å­˜ç›‘æŽ§çš„æµå¼æŸ¥è¯¢
        
        Args:
            filters: æŸ¥è¯¢è¿‡æ»¤æ¡ä»¶
            batch_size: æ‰¹æ¬¡å¤§å°
            order_by: æŽ’åºå­—æ®µ
            desc_order: æ˜¯å¦é™åº
            memory_limit_mb: å†…å­˜é™åˆ¶ï¼ˆMBï¼‰
            auto_optimize: æ˜¯å¦å¯ç”¨è‡ªåŠ¨ä¼˜åŒ–
            
        Yields:
            æŸ¥è¯¢ç»“æžœæ‰¹æ¬¡
        """
        if not self._streaming_enabled:
            GLOG.WARNING("Streaming not enabled, falling back to traditional query")
            yield from self._fallback_to_traditional_query(filters, batch_size, order_by, desc_order)
            return
        
        try:
            # å¯¼å…¥ç›‘æŽ§æ¨¡å—
            from ginkgo.data.streaming.session_context import streaming_session
            from ginkgo.data.streaming.monitoring import memory_monitor
            
            database_type = getattr(self.driver, '_db_type', 'unknown') if hasattr(self, 'driver') else 'unknown'
            
            # ä½¿ç”¨ä¼šè¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            with streaming_session(
                database_type=database_type,
                auto_optimize=auto_optimize
            ) as session_context:
                
                batch_count = 0
                current_batch_size = batch_size or 1000
                
                # å†…å­˜é™åˆ¶æ£€æŸ¥
                if memory_limit_mb:
                    initial_snapshot = memory_monitor.get_current_snapshot()
                    if initial_snapshot.process_mb > memory_limit_mb:
                        GLOG.WARNING(
                            f"Current memory usage ({initial_snapshot.process_mb:.1f}MB) "
                            f"exceeds limit ({memory_limit_mb}MB), reducing batch size"
                        )
                        current_batch_size = max(current_batch_size // 2, 100)
                
                for batch in self.stream_find(
                    filters=filters,
                    batch_size=current_batch_size,
                    order_by=order_by,
                    desc_order=desc_order
                ):
                    batch_count += 1
                    
                    # æ›´æ–°ä¼šè¯è¿›åº¦
                    from ginkgo.data.streaming.session_context import streaming_session_manager
                    
                    current_memory = memory_monitor.get_current_snapshot().process_mb
                    streaming_session_manager.update_session_progress(
                        session_context.session_id,
                        len(batch),
                        current_memory
                    )
                    
                    # å†…å­˜ä¼˜åŒ–æ£€æŸ¥
                    if auto_optimize and memory_limit_mb and current_memory > memory_limit_mb:
                        # åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
                        new_batch_size = max(current_batch_size // 2, 100)
                        if new_batch_size != current_batch_size:
                            GLOG.WARNING(
                                f"Memory limit exceeded ({current_memory:.1f}MB > {memory_limit_mb}MB), "
                                f"reducing batch size: {current_batch_size} -> {new_batch_size}"
                            )
                            current_batch_size = new_batch_size
                        
                        # å¼ºåˆ¶åžƒåœ¾å›žæ”¶
                        memory_monitor.force_garbage_collection()
                    
                    yield batch
                
                # è®°å½•æœ€ç»ˆç»Ÿè®¡
                final_metrics = streaming_session_manager.get_session_metrics(session_context.session_id)
                if final_metrics:
                    GLOG.INFO(
                        f"Streaming query completed: {final_metrics['records_processed']} records, "
                        f"{final_metrics['processing_rate']:.1f} records/sec, "
                        f"peak memory: {final_metrics['memory_peak_mb']:.1f}MB"
                    )
        
        except Exception as e:
            GLOG.ERROR(f"Monitored streaming query failed: {e}")
            # é™çº§åˆ°å¸¸è§„æµå¼æŸ¥è¯¢
            yield from self.stream_find(filters, batch_size, order_by, desc_order)
    
    def get_streaming_session_metrics(self) -> List[Dict[str, Any]]:
        """
        ðŸ†• èŽ·å–æµå¼æŸ¥è¯¢ä¼šè¯æŒ‡æ ‡
        
        Returns:
            æ´»è·ƒä¼šè¯æŒ‡æ ‡åˆ—è¡¨
        """
        try:
            from ginkgo.data.streaming.session_context import streaming_session_manager
            return streaming_session_manager.list_active_sessions()
        except Exception as e:
            GLOG.ERROR(f"Failed to get streaming session metrics: {e}")
            return []
    
    def get_memory_statistics(self) -> Dict[str, Any]:
        """
        ðŸ†• èŽ·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            å†…å­˜ç»Ÿè®¡å­—å…¸
        """
        try:
            from ginkgo.data.streaming.monitoring import memory_monitor, session_manager
            
            memory_stats = memory_monitor.get_memory_statistics()
            session_stats = session_manager.get_session_statistics()
            
            return {
                "memory": memory_stats,
                "sessions": session_stats,
                "timestamp": time.time()
            }
        except Exception as e:
            GLOG.ERROR(f"Failed to get memory statistics: {e}")
            return {"error": str(e)}
    
    def optimize_streaming_resources(self) -> Dict[str, Any]:
        """
        ðŸ†• æ‰‹åŠ¨ä¼˜åŒ–æµå¼æŸ¥è¯¢èµ„æº
        
        Returns:
            ä¼˜åŒ–ç»“æžœæŠ¥å‘Š
        """
        try:
            from ginkgo.data.streaming.monitoring import memory_monitor, session_manager
            
            optimization_report = {
                "timestamp": time.time(),
                "actions_taken": [],
                "before": {},
                "after": {}
            }
            
            # è®°å½•ä¼˜åŒ–å‰çŠ¶æ€
            optimization_report["before"] = {
                "memory_percent": memory_monitor.get_current_snapshot().percent,
                "active_sessions": len(session_manager.list_sessions(state=SessionState.ACTIVE)),
                "idle_sessions": len(session_manager.list_sessions(state=SessionState.IDLE))
            }
            
            # 1. å¼ºåˆ¶åžƒåœ¾å›žæ”¶
            gc_result = memory_monitor.force_garbage_collection()
            if gc_result.get("collected", 0) > 0:
                optimization_report["actions_taken"].append(
                    f"Garbage collection: freed {gc_result.get('freed_objects', 0)} objects"
                )
            
            # 2. æ¸…ç†è¿‡æœŸä¼šè¯
            from ginkgo.data.streaming.monitoring import SessionState
            idle_sessions = session_manager.list_sessions(state=SessionState.IDLE)
            cleaned_sessions = 0
            
            for session in idle_sessions:
                if session.idle_seconds > 300:  # 5åˆ†é’Ÿç©ºé—²
                    session_manager.close_session(session.session_id)
                    cleaned_sessions += 1
            
            if cleaned_sessions > 0:
                optimization_report["actions_taken"].append(
                    f"Cleaned {cleaned_sessions} idle sessions"
                )
            
            # è®°å½•ä¼˜åŒ–åŽçŠ¶æ€
            optimization_report["after"] = {
                "memory_percent": memory_monitor.get_current_snapshot().percent,
                "active_sessions": len(session_manager.list_sessions(state=SessionState.ACTIVE)),
                "idle_sessions": len(session_manager.list_sessions(state=SessionState.IDLE))
            }
            
            # è®¡ç®—ä¼˜åŒ–æ•ˆæžœ
            memory_improvement = (
                optimization_report["before"]["memory_percent"] - 
                optimization_report["after"]["memory_percent"]
            )
            optimization_report["memory_improvement_percent"] = memory_improvement
            
            GLOG.INFO(f"Resource optimization completed: {optimization_report}")
            return optimization_report
            
        except Exception as e:
            GLOG.ERROR(f"Failed to optimize streaming resources: {e}")
            return {"error": str(e)}
    
    def enable_memory_monitoring(self) -> bool:
        """
        ðŸ†• å¯ç”¨å†…å­˜ç›‘æŽ§
        
        Returns:
            æ˜¯å¦æˆåŠŸå¯ç”¨
        """
        try:
            from ginkgo.data.streaming.monitoring import memory_monitor, session_manager
            
            if not memory_monitor._monitoring:
                memory_monitor.start_monitoring()
                GLOG.INFO("Memory monitoring started")
            
            if not session_manager._cleanup_enabled:
                session_manager.start_cleanup()
                GLOG.INFO("Session cleanup started")
            
            return True
            
        except Exception as e:
            GLOG.ERROR(f"Failed to enable memory monitoring: {e}")
            return False
    
    def disable_memory_monitoring(self) -> bool:
        """
        ðŸ†• ç¦ç”¨å†…å­˜ç›‘æŽ§
        
        Returns:
            æ˜¯å¦æˆåŠŸç¦ç”¨
        """
        try:
            from ginkgo.data.streaming.monitoring import memory_monitor, session_manager
            
            memory_monitor.stop_monitoring()
            session_manager.stop_cleanup()
            
            GLOG.INFO("Memory monitoring and session cleanup stopped")
            return True
            
        except Exception as e:
            GLOG.ERROR(f"Failed to disable memory monitoring: {e}")
            return False
