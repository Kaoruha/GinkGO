#!/usr/bin/env python3
"""
TDDå¼€å‘åŠ©æ‰‹å·¥å…·

æä¾›TDDå¼€å‘è¿‡ç¨‹ä¸­çš„è‡ªåŠ¨åŒ–æ”¯æŒï¼š
1. Red-Green-Refactoræµç¨‹ç®¡ç†
2. æµ‹è¯•è¦†ç›–ç‡ç›‘æ§
3. Mockä½¿ç”¨åˆ†æ
4. TDDåº¦é‡æ”¶é›†

ä½¿ç”¨æ–¹æ³•ï¼š
python test/tools/tdd_helper.py --mode red --module order
python test/tools/tdd_helper.py --mode green --run-tests
python test/tools/tdd_helper.py --coverage-report
"""

import argparse
import subprocess
import sys
import os
from pathlib import Path
from typing import Dict, List, Optional
import json
import time
from datetime import datetime

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent.parent


class TDDHelper:
    """TDDå¼€å‘åŠ©æ‰‹"""

    def __init__(self):
        self.test_dir = PROJECT_ROOT / "test"
        self.src_dir = PROJECT_ROOT / "src"
        self.metrics_file = self.test_dir / "tdd_metrics.json"

    def run_red_phase(self, module: str, no_suggestions: bool = False) -> bool:
        """Redé˜¶æ®µï¼šç¼–å†™å¤±è´¥æµ‹è¯•"""
        print(f"ğŸ”´ Redé˜¶æ®µï¼šä¸ºæ¨¡å— {module} ç¼–å†™å¤±è´¥æµ‹è¯•")

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æµ‹è¯•æ–‡ä»¶ - æ”¯æŒæ–°çš„æ¨¡å—åŒ–ç»“æ„
        module_parts = module.split('_')

        if len(module_parts) >= 3:
            # æ¨¡å—åŒ–è·¯å¾„: trading_entities_position
            main_module, sub_module, class_name = module_parts[:3]
            test_patterns = [
                f"test/{main_module}/{sub_module}/test_{class_name}.py",
                f"test/data/models/test_{class_name}_model.py",
                f"test/data/crud/test_{class_name}_crud.py",
                f"test/integration/test_{class_name}_*.py"
            ]
        else:
            # ä¼ ç»Ÿæ¨¡å¼å…¼å®¹
            test_patterns = [
                f"test/**/test_{module}.py",
                f"test/**/test_{module}_*.py",
                f"test/integration/**/test_{module}_*.py"
            ]

        found_tests = []
        for pattern in test_patterns:
            found_tests.extend(list(PROJECT_ROOT.glob(pattern)))

        if not found_tests:
            print(f"âŒ æœªæ‰¾åˆ°æ¨¡å— {module} çš„TDDæµ‹è¯•æ–‡ä»¶")
        else:
            print(f"âœ… æ‰¾åˆ° {len(found_tests)} ä¸ªæµ‹è¯•æ–‡ä»¶:")
            for test_file in found_tests:
                print(f"   - {test_file.relative_to(PROJECT_ROOT)}")

        # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦æä¾›åˆ›å»ºå»ºè®®
        if not no_suggestions:
            self._suggest_test_creation(module, found_tests)
        else:
            print("ğŸ”§ å·²è·³è¿‡æ–‡ä»¶åˆ›å»ºå»ºè®®")

        if found_tests:
            # è¿è¡Œæµ‹è¯•éªŒè¯å®ƒä»¬å¤±è´¥
            return self._run_tests_expect_failure(found_tests)
        else:
            return True

    def run_green_phase(self, run_tests: bool = True) -> bool:
        """Greené˜¶æ®µï¼šå®ç°æœ€å°å¯ç”¨ä»£ç """
        print("ğŸŸ¢ Greené˜¶æ®µï¼šè¿è¡Œæµ‹è¯•éªŒè¯å®ç°")

        if run_tests:
            success = self._run_all_tdd_tests()
            # Greené˜¶æ®µï¼šå³ä½¿æµ‹è¯•å¤±è´¥ä¹Ÿè¿”å›æˆåŠŸï¼Œå› ä¸ºè¿™æ˜¯å¼€å‘è¿‡ç¨‹çš„ä¸€éƒ¨åˆ†
            print("ğŸ’¡ æç¤ºï¼šå¦‚æœæµ‹è¯•å¤±è´¥ï¼Œè¯·ç»§ç»­å®ç°ä»£ç ç›´åˆ°æµ‹è¯•é€šè¿‡")
            return True

        print("æç¤ºï¼šå®ç°æœ€å°å¯ç”¨ä»£ç ï¼Œç„¶åè¿è¡Œ --mode green --run-tests")
        return True

    def run_refactor_phase(self) -> bool:
        """Refactoré˜¶æ®µï¼šé‡æ„ä»£ç ä¿æŒæµ‹è¯•é€šè¿‡"""
        print("ğŸ”„ Refactoré˜¶æ®µï¼šé‡æ„å¹¶éªŒè¯æµ‹è¯•ä»ç„¶é€šè¿‡")

        # è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
        success = self._run_all_tdd_tests()

        if success:
            # è¿è¡Œä»£ç è´¨é‡æ£€æŸ¥
            self._run_code_quality_checks()

        return success

    def generate_coverage_report(self) -> bool:
        """ç”Ÿæˆæµ‹è¯•è¦†ç›–ç‡æŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆæµ‹è¯•è¦†ç›–ç‡æŠ¥å‘Š")

        try:
            # è¿è¡Œè¦†ç›–ç‡æµ‹è¯•
            cmd = [
                "python", "-m", "pytest",
                "test/",
                "--cov=src/ginkgo",
                "--cov-report=html:test/htmlcov",
                "--cov-report=term-missing",
                "-q"
            ]

            result = subprocess.run(cmd, cwd=PROJECT_ROOT, capture_output=True, text=True)

            if result.returncode == 0:
                print("âœ… è¦†ç›–ç‡æŠ¥å‘Šå·²ç”Ÿæˆ: test/htmlcov/index.html")
                self._analyze_coverage_report(result.stdout)
                return True
            else:
                print(f"âŒ ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Šå¤±è´¥: {result.stderr}")
                return False

        except Exception as e:
            print(f"âŒ è¦†ç›–ç‡åˆ†æé”™è¯¯: {e}")
            return False

    def analyze_mock_usage(self) -> Dict:
        """åˆ†æMockä½¿ç”¨æƒ…å†µ"""
        print("ğŸ” åˆ†æMockä½¿ç”¨æƒ…å†µ")

        mock_analysis = {
            "total_test_files": 0,
            "files_with_mock": 0,
            "mock_usage_ratio": 0.0,
            "mock_patterns": {},
            "recommendations": []
        }

        # æ‰«ææµ‹è¯•æ–‡ä»¶
        test_files = list(self.test_dir.glob("**/*.py"))
        mock_usage = {}

        for test_file in test_files:
            if test_file.name.startswith("test_"):
                mock_analysis["total_test_files"] += 1

                try:
                    with open(test_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # æ£€æµ‹Mockä½¿ç”¨æ¨¡å¼
                    mock_patterns = [
                        "from unittest.mock import",
                        "@patch(",
                        "@mock.patch",
                        "Mock(",
                        "MagicMock(",
                        ".return_value =",
                        "mock_"
                    ]

                    file_mock_count = 0
                    for pattern in mock_patterns:
                        file_mock_count += content.count(pattern)

                    if file_mock_count > 0:
                        mock_analysis["files_with_mock"] += 1
                        mock_usage[str(test_file.relative_to(self.test_dir))] = file_mock_count

                except Exception as e:
                    print(f"è­¦å‘Šï¼šæ— æ³•è¯»å–æ–‡ä»¶ {test_file}: {e}")

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if mock_analysis["total_test_files"] > 0:
            mock_analysis["mock_usage_ratio"] = mock_analysis["files_with_mock"] / mock_analysis["total_test_files"]

        mock_analysis["mock_patterns"] = mock_usage

        # ç”Ÿæˆå»ºè®®
        if mock_analysis["mock_usage_ratio"] > 0.5:
            mock_analysis["recommendations"].append("Mockä½¿ç”¨ç‡è¿‡é«˜ï¼Œè€ƒè™‘ä½¿ç”¨æ›´å¤šé›†æˆæµ‹è¯•")

        if mock_analysis["mock_usage_ratio"] > 0.7:
            mock_analysis["recommendations"].append("ä¸¥é‡ä¾èµ–Mockï¼Œå»ºè®®é‡æ„ä¸ºæµ‹è¯•çœŸå®å¯¹è±¡äº¤äº’")

        # æ˜¾ç¤ºç»“æœ
        print(f"ğŸ“ˆ Mockä½¿ç”¨åˆ†æç»“æœ:")
        print(f"   æ€»æµ‹è¯•æ–‡ä»¶: {mock_analysis['total_test_files']}")
        print(f"   ä½¿ç”¨Mockæ–‡ä»¶: {mock_analysis['files_with_mock']}")
        print(f"   Mockä½¿ç”¨ç‡: {mock_analysis['mock_usage_ratio']:.1%}")

        if mock_analysis["recommendations"]:
            print("ğŸ’¡ å»ºè®®:")
            for rec in mock_analysis["recommendations"]:
                print(f"   - {rec}")

        return mock_analysis

    def collect_tdd_metrics(self) -> Dict:
        """æ”¶é›†TDDåº¦é‡æ•°æ®"""
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "test_counts": self._count_tests(),
            "coverage": self._get_coverage_metrics(),
            "mock_usage": self.analyze_mock_usage(),
            "test_execution_time": self._measure_test_execution_time(),
            "red_green_refactor_cycles": self._count_tdd_cycles()
        }

        # ä¿å­˜åº¦é‡æ•°æ®
        try:
            with open(self.metrics_file, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)

            print(f"ğŸ“Š TDDåº¦é‡æ•°æ®å·²ä¿å­˜: {self.metrics_file}")

        except Exception as e:
            print(f"âŒ ä¿å­˜åº¦é‡æ•°æ®å¤±è´¥: {e}")

        return metrics

    def _run_tests_expect_failure(self, test_files: List[Path]) -> bool:
        """è¿è¡Œæµ‹è¯•æœŸæœ›å¤±è´¥ï¼ˆRedé˜¶æ®µéªŒè¯ï¼‰"""
        print("éªŒè¯æµ‹è¯•å¤±è´¥...")

        failed_count = 0
        for test_file in test_files:
            cmd = ["python", "-m", "pytest", str(test_file), "-v", "--tb=short"]
            result = subprocess.run(cmd, cwd=PROJECT_ROOT, capture_output=True, text=True)

            if result.returncode != 0:
                failed_count += 1
                print(f"âœ… {test_file.name} - æµ‹è¯•æŒ‰é¢„æœŸå¤±è´¥")
            else:
                print(f"âš ï¸ {test_file.name} - æµ‹è¯•æ„å¤–é€šè¿‡")

        if failed_count == len(test_files):
            print("ğŸ”´ Redé˜¶æ®µéªŒè¯æˆåŠŸï¼šæ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥äº†")
        else:
            print("âš ï¸ Redé˜¶æ®µæé†’ï¼šæŸäº›æµ‹è¯•æ„å¤–é€šè¿‡ï¼Œè¯·æ£€æŸ¥æµ‹è¯•å†…å®¹")
            print("ğŸ’¡ æç¤ºï¼šRedé˜¶æ®µæœŸæœ›æµ‹è¯•å¤±è´¥ï¼Œå¦‚æœé€šè¿‡è¯·ç¡®è®¤æµ‹è¯•é€»è¾‘æ˜¯å¦æ­£ç¡®")

        # Redé˜¶æ®µæ€»æ˜¯è¿”å›æˆåŠŸï¼Œè®©å¼€å‘æµç¨‹ç»§ç»­
        return True

    def _run_all_tdd_tests(self) -> bool:
        """è¿è¡Œæ‰€æœ‰TDDæµ‹è¯•"""
        cmd = [
            "python", "-m", "pytest",
            "test/",
            "-m", "tdd",
            "-v",
            "--tb=short"
        ]

        print("è¿è¡ŒTDDæµ‹è¯•...")
        result = subprocess.run(cmd, cwd=PROJECT_ROOT)

        if result.returncode == 0:
            print("âœ… æ‰€æœ‰TDDæµ‹è¯•é€šè¿‡")
            return True
        else:
            print("âŒ TDDæµ‹è¯•å¤±è´¥")
            return False

    def _run_code_quality_checks(self):
        """è¿è¡Œä»£ç è´¨é‡æ£€æŸ¥"""
        print("ğŸ” è¿è¡Œä»£ç è´¨é‡æ£€æŸ¥...")

        # æ£€æŸ¥æ˜¯å¦æœ‰ä»£ç æ ¼å¼åŒ–å·¥å…·
        quality_tools = [
            ("flake8", ["flake8", "src/"]),
            ("black", ["black", "--check", "src/"]),
            ("isort", ["isort", "--check-only", "src/"])
        ]

        for tool_name, cmd in quality_tools:
            try:
                result = subprocess.run(cmd, cwd=PROJECT_ROOT, capture_output=True, text=True)
                if result.returncode == 0:
                    print(f"âœ… {tool_name} æ£€æŸ¥é€šè¿‡")
                else:
                    print(f"âš ï¸ {tool_name} å‘ç°é—®é¢˜")
            except FileNotFoundError:
                print(f"â„¹ï¸ {tool_name} æœªå®‰è£…ï¼Œè·³è¿‡æ£€æŸ¥")

    def _suggest_test_creation(self, module: str, existing_files: list = None):
        """å»ºè®®åˆ›å»ºæµ‹è¯•æ–‡ä»¶ - åŸºäºGinkgoæ¨¡å—ç»“æ„"""
        existing_files = existing_files or []
        existing_paths = {str(f.relative_to(PROJECT_ROOT)) for f in existing_files}

        # è§£ææ¨¡å—è·¯å¾„ï¼Œæ”¯æŒå¦‚: trading_entities_position, data_models_position
        module_parts = module.split('_')

        if len(module_parts) >= 2:
            # æ¨¡å—åŒ–è·¯å¾„: trading_entities_position -> trading/entities/test_position.py
            main_module = module_parts[0]  # trading
            sub_module = module_parts[1]   # entities
            class_name = module_parts[2] if len(module_parts) > 2 else "component"  # position

            suggestions = [
                f"test/{main_module}/{sub_module}/test_{class_name}.py - {class_name}æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•",
            ]

            # æ ¹æ®æ¨¡å—ç±»å‹æ·»åŠ é¢å¤–å»ºè®®
            if main_module == "trading" and sub_module == "entities":
                suggestions.extend([
                    f"test/data/models/test_{class_name}_model.py - {class_name}æ•°æ®æ¨¡å‹æµ‹è¯•",
                    f"test/data/crud/test_{class_name}_crud.py - {class_name}æ•°æ®æ“ä½œæµ‹è¯•",
                    f"test/integration/test_{class_name}_integration.py - {class_name}é›†æˆæµ‹è¯•"
                ])
            elif main_module == "trading" and "risk" in sub_module:
                suggestions.append(f"test/integration/test_{class_name}_risk_integration.py - é£æ§é›†æˆæµ‹è¯•")
            elif main_module == "libs":
                # libsæ¨¡å—æµ‹è¯•å»ºè®®
                if sub_module == "core":
                    suggestions.extend([
                        f"test/integration/test_{class_name}_libs_integration.py - {class_name}æ ¸å¿ƒåº“é›†æˆæµ‹è¯•"
                    ])
                elif sub_module == "containers":
                    suggestions.extend([
                        f"test/integration/test_{class_name}_container_integration.py - {class_name}å®¹å™¨é›†æˆæµ‹è¯•"
                    ])
            elif main_module == "features":
                # featuresæ¨¡å—æµ‹è¯•å»ºè®®
                if sub_module == "engines":
                    suggestions.extend([
                        f"test/integration/test_{class_name}_feature_integration.py - {class_name}ç‰¹å¾å¼•æ“é›†æˆæµ‹è¯•"
                    ])
                elif sub_module == "services":
                    suggestions.extend([
                        f"test/integration/test_{class_name}_service_integration.py - {class_name}ç‰¹å¾æœåŠ¡é›†æˆæµ‹è¯•"
                    ])
            elif main_module == "data":
                # dataæ¨¡å—æµ‹è¯•å»ºè®®
                if sub_module == "drivers":
                    suggestions.extend([
                        f"test/integration/test_{class_name}_driver_integration.py - {class_name}æ•°æ®é©±åŠ¨é›†æˆæµ‹è¯•"
                    ])
                elif sub_module == "streaming":
                    suggestions.extend([
                        f"test/integration/test_{class_name}_streaming_integration.py - {class_name}æµå¼æ•°æ®é›†æˆæµ‹è¯•"
                    ])
            elif main_module == "quant_ml":
                # é‡åŒ–MLæ¨¡å—æµ‹è¯•å»ºè®®
                suggestions.extend([
                    f"test/integration/test_{class_name}_ml_integration.py - {class_name}æœºå™¨å­¦ä¹ é›†æˆæµ‹è¯•"
                ])
            elif main_module == "client":
                # CLIå®¢æˆ·ç«¯æµ‹è¯•å»ºè®®
                suggestions.extend([
                    f"test/integration/test_{class_name}_cli_integration.py - {class_name}CLIé›†æˆæµ‹è¯•"
                ])

        else:
            # ä¼ ç»Ÿæ¨¡å¼å…¼å®¹
            suggestions = [
                f"test/trading/entities/test_{module}.py - {module}å®ä½“æµ‹è¯•",
                f"test/data/models/test_{module}_model.py - {module}æ¨¡å‹æµ‹è¯•",
                f"test/integration/test_{module}_integration.py - {module}é›†æˆæµ‹è¯•"
            ]

        # è¿‡æ»¤å‡ºä¸å­˜åœ¨çš„æ–‡ä»¶
        new_suggestions = []
        for suggestion in suggestions:
            file_path = suggestion.split(' - ')[0]
            if file_path not in existing_paths:
                new_suggestions.append(suggestion)

        if new_suggestions:
            print("ğŸ’¡ å»ºè®®åˆ›å»ºä»¥ä¸‹æµ‹è¯•æ–‡ä»¶:")
            for i, suggestion in enumerate(new_suggestions, 1):
                print(f"   {i}. {suggestion}")

            # äº¤äº’å¼åˆ›å»ºé€‰é¡¹
            print("\né€‰æ‹©è¦åˆ›å»ºçš„æ–‡ä»¶ (è¾“å…¥æ•°å­—ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œæˆ–æŒ‰å›è½¦è·³è¿‡):")
            try:
                user_input = input(">>> ").strip()
                if user_input:
                    self._create_selected_files(user_input, new_suggestions)
            except (KeyboardInterrupt, EOFError):
                print("\nè·³è¿‡æ–‡ä»¶åˆ›å»º")
        else:
            print("âœ… æ‰€æœ‰æ¨èçš„æµ‹è¯•æ–‡ä»¶éƒ½å·²å­˜åœ¨")

    def _create_selected_files(self, user_input: str, suggestions: list):
        """æ ¹æ®ç”¨æˆ·é€‰æ‹©åˆ›å»ºæµ‹è¯•æ–‡ä»¶"""
        try:
            # è§£æç”¨æˆ·è¾“å…¥
            selections = [int(x.strip()) for x in user_input.split(',')]

            for selection in selections:
                if 1 <= selection <= len(suggestions):
                    suggestion = suggestions[selection - 1]
                    # æå–æ–‡ä»¶è·¯å¾„ (å»æ‰æè¿°éƒ¨åˆ†)
                    file_path = suggestion.split(' - ')[0].replace('test/', '')
                    self._create_test_file(file_path, suggestion)
                else:
                    print(f"âŒ æ— æ•ˆé€‰æ‹©: {selection}")

        except ValueError:
            print("âŒ è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥æ•°å­—")

    def _create_test_file(self, file_path: str, description: str):
        """åˆ›å»ºå•ä¸ªæµ‹è¯•æ–‡ä»¶"""
        full_path = self.test_dir / file_path

        # åˆ›å»ºç›®å½•
        full_path.parent.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºæ–‡ä»¶
        if not full_path.exists():
            with open(full_path, 'w', encoding='utf-8') as f:
                f.write(self._get_test_template(file_path))
            print(f"âœ… å·²åˆ›å»º: {file_path}")
        else:
            print(f"â„¹ï¸ æ–‡ä»¶å·²å­˜åœ¨: {file_path}")

    def _get_test_template(self, file_path: str) -> str:
        """è·å–æµ‹è¯•æ–‡ä»¶æ¨¡æ¿"""
        class_name = file_path.split('/')[-1].replace('test_', '').replace('.py', '')

        template = f'''"""
{class_name}æµ‹è¯•

TDDé©±åŠ¨å¼€å‘æµ‹è¯•æ–‡ä»¶
"""
import pytest
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root / "src"))


class Test{class_name.title()}TDD:
    """{class_name}ç±»TDDæµ‹è¯•å¥—ä»¶"""

    def test_{class_name.lower()}_placeholder(self):
        """
        å ä½æµ‹è¯• - è¯·æ ¹æ®éœ€æ±‚ç¼–å†™å…·ä½“æµ‹è¯•
        """
        # TODO: ç¼–å†™å…·ä½“æµ‹è¯•ç”¨ä¾‹
        assert True  # ä¸´æ—¶å ä½
'''
        return template

    def _count_tests(self) -> Dict:
        """ç»Ÿè®¡æµ‹è¯•æ•°é‡"""
        counts = {"total": 0, "tdd": 0, "integration": 0, "unit": 0}

        test_files = list(self.test_dir.glob("**/*.py"))
        for test_file in test_files:
            if test_file.name.startswith("test_"):
                counts["total"] += 1

                # ç»Ÿè®¡æµ‹è¯•ç±»å‹
                if "_tdd.py" in test_file.name:
                    counts["tdd"] += 1
                elif "integration" in str(test_file):
                    counts["integration"] += 1
                else:
                    counts["unit"] += 1

        return counts

    def _get_coverage_metrics(self) -> Dict:
        """è·å–è¦†ç›–ç‡åº¦é‡"""
        # è¿™é‡Œå¯ä»¥è§£æcoverage.pyçš„è¾“å‡º
        return {"coverage_percentage": 0, "uncovered_lines": 0}

    def _measure_test_execution_time(self) -> float:
        """æµ‹é‡æµ‹è¯•æ‰§è¡Œæ—¶é—´"""
        start_time = time.time()

        cmd = ["python", "-m", "pytest", "test/", "-q", "--tb=no"]
        subprocess.run(cmd, cwd=PROJECT_ROOT, capture_output=True)

        return time.time() - start_time

    def _count_tdd_cycles(self) -> int:
        """ç»Ÿè®¡TDDå¾ªç¯æ¬¡æ•°ï¼ˆä»gitæäº¤å†å²åˆ†æï¼‰"""
        try:
            cmd = ["git", "log", "--oneline", "--grep=TDD", "--grep=Red", "--grep=Green", "--grep=Refactor"]
            result = subprocess.run(cmd, cwd=PROJECT_ROOT, capture_output=True, text=True)
            return len(result.stdout.strip().split('\n')) if result.stdout.strip() else 0
        except:
            return 0

    def _analyze_coverage_report(self, coverage_output: str):
        """åˆ†æè¦†ç›–ç‡æŠ¥å‘Š"""
        lines = coverage_output.split('\n')
        for line in lines:
            if "TOTAL" in line:
                print(f"ğŸ“Š {line}")
                break


def main():
    parser = argparse.ArgumentParser(description="TDDå¼€å‘åŠ©æ‰‹å·¥å…·")
    parser.add_argument("--mode", choices=["red", "green", "refactor"], help="TDDé˜¶æ®µ")
    parser.add_argument("--module", help="æ¨¡å—åç§°ï¼ˆç”¨äºRedé˜¶æ®µï¼‰")
    parser.add_argument("--no-suggestions", action="store_true", help="è·³è¿‡æ–‡ä»¶åˆ›å»ºå»ºè®®")
    parser.add_argument("--run-tests", action="store_true", help="è¿è¡Œæµ‹è¯•")
    parser.add_argument("--coverage-report", action="store_true", help="ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š")
    parser.add_argument("--analyze-mock", action="store_true", help="åˆ†æMockä½¿ç”¨")
    parser.add_argument("--collect-metrics", action="store_true", help="æ”¶é›†TDDåº¦é‡")

    args = parser.parse_args()

    helper = TDDHelper()

    if args.mode == "red":
        if not args.module:
            print("âŒ Redé˜¶æ®µéœ€è¦æŒ‡å®š--moduleå‚æ•°")
            sys.exit(1)
        success = helper.run_red_phase(args.module, args.no_suggestions)
        sys.exit(0 if success else 1)

    elif args.mode == "green":
        success = helper.run_green_phase(args.run_tests)
        sys.exit(0 if success else 1)

    elif args.mode == "refactor":
        success = helper.run_refactor_phase()
        sys.exit(0 if success else 1)

    elif args.coverage_report:
        success = helper.generate_coverage_report()
        sys.exit(0 if success else 1)

    elif args.analyze_mock:
        helper.analyze_mock_usage()
        sys.exit(0)

    elif args.collect_metrics:
        helper.collect_tdd_metrics()
        sys.exit(0)

    else:
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()